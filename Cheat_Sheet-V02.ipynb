{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kevin's Pandas' Crib Sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whays to get help:\n",
    "\n",
    "| Command           | Function |\n",
    "|---------------    |----------|\n",
    "| Auto complete     | To get options |\n",
    "| Hover             |     To get docstring |\n",
    "| dir('func')       | Shows all method and function calls |\n",
    "| help('func')      | See documentation |\n",
    "| ?                 | Access documentation |\n",
    "| ??                | Access source code |\n",
    "| %lsmagic          | List available magic commands |\n",
    "| %quickref         | Magic quick refernce sheet |\n",
    "\n",
    "Good guide [here](https://problemsolvingwithpython.com/02-Jupyter-Notebooks/02.07-Getting-Help-in-a-Jupyter-Notebook/)\n",
    "\n",
    "Good tips, but focused on Jupyter Notebook in browser [here](https://towardsdatascience.com/15-tips-and-tricks-for-jupyter-notebook-that-will-ease-your-coding-experience-e469207ac95c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a consolidation notes and examples from:\n",
    "> Coreys MSchafer's Pandas videos [here](https://www.youtube.com/playlist?list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS) \n",
    "\n",
    "and \n",
    "> Hands on Data Analysis by Stefanie Molin\n",
    "All data in examples and exercises available [here](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition)\n",
    "\n",
    "Version 2.1W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set-Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Main Dataset Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Functions that create Example Datasets for use later \n",
    "\n",
    "def mk_dictionary(x):\n",
    "    if x == \"people\":\n",
    "        dictionary = {\n",
    "            'first': ['Corey', 'Jane', 'Janey', 'John', 'Jimmy'], \n",
    "            'last': ['Schafer', 'Doe', 'Doe', 'Doe', 'Doe'], \n",
    "            'email': [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JaneyDoe@email.com','JohnDoe@email.com', 'JimmyDoe@email.com']\n",
    "        }\n",
    "    elif x == 'people2':\n",
    "        dictionary = {\n",
    "            'first': ['Tony', 'Steve'], \n",
    "            'last': ['Stark', 'Rogers'], \n",
    "            'email': ['IronMan@avenge.com', 'Cap@avenge.com']\n",
    "        }\n",
    "    # Set-up some dirty data  \n",
    "    elif x == 'dirty':\n",
    "        dictionary = {\n",
    "    'first': ['Corey', 'Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'], \n",
    "    'last': ['Schafer', 'Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'], \n",
    "    'email': ['CoreyMSchafer@gmail.com','CoreyCORRUPTSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],\n",
    "    'age': ['33', '333', '55', '63', '36', None, None, 'Missing']\n",
    "    }\n",
    "    elif x == 'weather':\n",
    "        dictionary = big_dictionary('weather')\n",
    "    elif x == 'stations':\n",
    "        dictionary = {\n",
    "            'id': {0: 'GHCND:US1CTFR0022', 4: 'GHCND:US1NJBG0003', 278: 'GHCND:USW00094789'},\n",
    "            'name': {0: 'STAMFORD 2.6 SSW, CT US', 4: 'TENAFLY 1.3 W, NJ US', 278: 'JFK INTERNATIONAL AIRPORT, NY US'},\n",
    "            'latitude': {0: 41.0641, 4: 40.91467, 278: 40.63915},\n",
    "            'longitude': {0: -73.577, 4: -73.9775, 278: -73.76401},\n",
    "            'elevation': {0: 36.6, 4: 21.6, 278: 3.4}\n",
    "        }\n",
    "    else:\n",
    "        print(f'!!!!! mk_dictionary called wih invalid parameter !!!!')\n",
    "        raise SystemExit\n",
    "    return dictionary \n",
    "\n",
    "def mk_dataframe(x):\n",
    "    df =  pd.DataFrame(mk_dictionary(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "# people  = mk_dictionary('people')\n",
    "# people2 = mk_dictionary('people2')\n",
    "\n",
    "\n",
    "# df = mk_dataframe('stations')\n",
    "# df = mk_dataframe('people')\n",
    "# df2 = mk_dataframe('people2')\n",
    "# dirty_df = mk_dataframe('dirty')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Weather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3.1 DONT OPEN Very Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>datatype</th>\n",
       "      <th>station</th>\n",
       "      <th>attributes</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58914</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:US1NJBG0003</td>\n",
       "      <td>,,N,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59144</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59145</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59155</th>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:US1NJBG0003</td>\n",
       "      <td>,,N,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59372</th>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65128</th>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65129</th>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65139</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:US1NJBG0003</td>\n",
       "      <td>,,N,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65350</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65351</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>GHCND:USW00094789</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date datatype            station attributes  value\n",
       "58914  2018-10-01     SNOW  GHCND:US1NJBG0003       ,,N,    0.0\n",
       "59144  2018-10-01     SNOW  GHCND:USW00094789       ,,W,    0.0\n",
       "59145  2018-10-01     SNWD  GHCND:USW00094789   ,,W,2400    0.0\n",
       "59155  2018-10-02     SNOW  GHCND:US1NJBG0003       ,,N,    0.0\n",
       "59372  2018-10-02     SNOW  GHCND:USW00094789       ,,W,    0.0\n",
       "...           ...      ...                ...        ...    ...\n",
       "65128  2018-10-30     SNOW  GHCND:USW00094789       ,,W,    0.0\n",
       "65129  2018-10-30     SNWD  GHCND:USW00094789   ,,W,2400    0.0\n",
       "65139  2018-10-31     SNOW  GHCND:US1NJBG0003       ,,N,    0.0\n",
       "65350  2018-10-31     SNOW  GHCND:USW00094789       ,,W,    0.0\n",
       "65351  2018-10-31     SNWD  GHCND:USW00094789   ,,W,2400    0.0\n",
       "\n",
       "[79 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuff in here?\n",
    "def big_dictionary(x):\n",
    "    if x == 'weather':\n",
    "        return {    \n",
    "  'date': {58914: '2018-10-01',\n",
    "  59144: '2018-10-01',\n",
    "  59145: '2018-10-01',\n",
    "  59155: '2018-10-02',\n",
    "  59372: '2018-10-02',\n",
    "  59373: '2018-10-02',\n",
    "  59580: '2018-10-03',\n",
    "  59581: '2018-10-03',\n",
    "  59592: '2018-10-04',\n",
    "  59802: '2018-10-04',\n",
    "  59803: '2018-10-04',\n",
    "  59996: '2018-10-05',\n",
    "  59997: '2018-10-05',\n",
    "  60007: '2018-10-06',\n",
    "  60206: '2018-10-06',\n",
    "  60207: '2018-10-06',\n",
    "  60405: '2018-10-07',\n",
    "  60406: '2018-10-07',\n",
    "  60416: '2018-10-08',\n",
    "  60605: '2018-10-08',\n",
    "  60606: '2018-10-08',\n",
    "  60808: '2018-10-09',\n",
    "  60809: '2018-10-09',\n",
    "  60819: '2018-10-10',\n",
    "  61011: '2018-10-10',\n",
    "  61012: '2018-10-10',\n",
    "  61024: '2018-10-11',\n",
    "  61238: '2018-10-11',\n",
    "  61239: '2018-10-11',\n",
    "  61451: '2018-10-12',\n",
    "  61452: '2018-10-12',\n",
    "  61632: '2018-10-13',\n",
    "  61633: '2018-10-13',\n",
    "  61822: '2018-10-14',\n",
    "  61823: '2018-10-14',\n",
    "  62022: '2018-10-15',\n",
    "  62023: '2018-10-15',\n",
    "  62214: '2018-10-16',\n",
    "  62215: '2018-10-16',\n",
    "  62226: '2018-10-17',\n",
    "  62423: '2018-10-17',\n",
    "  62424: '2018-10-17',\n",
    "  62434: '2018-10-18',\n",
    "  62626: '2018-10-18',\n",
    "  62627: '2018-10-18',\n",
    "  62637: '2018-10-19',\n",
    "  62848: '2018-10-19',\n",
    "  62849: '2018-10-19',\n",
    "  63031: '2018-10-20',\n",
    "  63032: '2018-10-20',\n",
    "  63221: '2018-10-21',\n",
    "  63222: '2018-10-21',\n",
    "  63233: '2018-10-22',\n",
    "  63440: '2018-10-22',\n",
    "  63441: '2018-10-22',\n",
    "  63451: '2018-10-23',\n",
    "  63665: '2018-10-23',\n",
    "  63666: '2018-10-23',\n",
    "  63676: '2018-10-24',\n",
    "  63871: '2018-10-24',\n",
    "  63872: '2018-10-24',\n",
    "  63882: '2018-10-25',\n",
    "  64092: '2018-10-25',\n",
    "  64093: '2018-10-25',\n",
    "  64103: '2018-10-26',\n",
    "  64319: '2018-10-26',\n",
    "  64320: '2018-10-26',\n",
    "  64514: '2018-10-27',\n",
    "  64515: '2018-10-27',\n",
    "  64715: '2018-10-28',\n",
    "  64716: '2018-10-28',\n",
    "  64923: '2018-10-29',\n",
    "  64924: '2018-10-29',\n",
    "  64934: '2018-10-30',\n",
    "  65128: '2018-10-30',\n",
    "  65129: '2018-10-30',\n",
    "  65139: '2018-10-31',\n",
    "  65350: '2018-10-31',\n",
    "  65351: '2018-10-31'},\n",
    " 'datatype': {58914: 'SNOW',\n",
    "  59144: 'SNOW',\n",
    "  59145: 'SNWD',\n",
    "  59155: 'SNOW',\n",
    "  59372: 'SNOW',\n",
    "  59373: 'SNWD',\n",
    "  59580: 'SNOW',\n",
    "  59581: 'SNWD',\n",
    "  59592: 'SNOW',\n",
    "  59802: 'SNOW',\n",
    "  59803: 'SNWD',\n",
    "  59996: 'SNOW',\n",
    "  59997: 'SNWD',\n",
    "  60007: 'SNOW',\n",
    "  60206: 'SNOW',\n",
    "  60207: 'SNWD',\n",
    "  60405: 'SNOW',\n",
    "  60406: 'SNWD',\n",
    "  60416: 'SNOW',\n",
    "  60605: 'SNOW',\n",
    "  60606: 'SNWD',\n",
    "  60808: 'SNOW',\n",
    "  60809: 'SNWD',\n",
    "  60819: 'SNOW',\n",
    "  61011: 'SNOW',\n",
    "  61012: 'SNWD',\n",
    "  61024: 'SNOW',\n",
    "  61238: 'SNOW',\n",
    "  61239: 'SNWD',\n",
    "  61451: 'SNOW',\n",
    "  61452: 'SNWD',\n",
    "  61632: 'SNOW',\n",
    "  61633: 'SNWD',\n",
    "  61822: 'SNOW',\n",
    "  61823: 'SNWD',\n",
    "  62022: 'SNOW',\n",
    "  62023: 'SNWD',\n",
    "  62214: 'SNOW',\n",
    "  62215: 'SNWD',\n",
    "  62226: 'SNOW',\n",
    "  62423: 'SNOW',\n",
    "  62424: 'SNWD',\n",
    "  62434: 'SNOW',\n",
    "  62626: 'SNOW',\n",
    "  62627: 'SNWD',\n",
    "  62637: 'SNOW',\n",
    "  62848: 'SNOW',\n",
    "  62849: 'SNWD',\n",
    "  63031: 'SNOW',\n",
    "  63032: 'SNWD',\n",
    "  63221: 'SNOW',\n",
    "  63222: 'SNWD',\n",
    "  63233: 'SNOW',\n",
    "  63440: 'SNOW',\n",
    "  63441: 'SNWD',\n",
    "  63451: 'SNOW',\n",
    "  63665: 'SNOW',\n",
    "  63666: 'SNWD',\n",
    "  63676: 'SNOW',\n",
    "  63871: 'SNOW',\n",
    "  63872: 'SNWD',\n",
    "  63882: 'SNOW',\n",
    "  64092: 'SNOW',\n",
    "  64093: 'SNWD',\n",
    "  64103: 'SNOW',\n",
    "  64319: 'SNOW',\n",
    "  64320: 'SNWD',\n",
    "  64514: 'SNOW',\n",
    "  64515: 'SNWD',\n",
    "  64715: 'SNOW',\n",
    "  64716: 'SNWD',\n",
    "  64923: 'SNOW',\n",
    "  64924: 'SNWD',\n",
    "  64934: 'SNOW',\n",
    "  65128: 'SNOW',\n",
    "  65129: 'SNWD',\n",
    "  65139: 'SNOW',\n",
    "  65350: 'SNOW',\n",
    "  65351: 'SNWD'},\n",
    " 'station': {58914: 'GHCND:US1NJBG0003',\n",
    "  59144: 'GHCND:USW00094789',\n",
    "  59145: 'GHCND:USW00094789',\n",
    "  59155: 'GHCND:US1NJBG0003',\n",
    "  59372: 'GHCND:USW00094789',\n",
    "  59373: 'GHCND:USW00094789',\n",
    "  59580: 'GHCND:USW00094789',\n",
    "  59581: 'GHCND:USW00094789',\n",
    "  59592: 'GHCND:US1NJBG0003',\n",
    "  59802: 'GHCND:USW00094789',\n",
    "  59803: 'GHCND:USW00094789',\n",
    "  59996: 'GHCND:USW00094789',\n",
    "  59997: 'GHCND:USW00094789',\n",
    "  60007: 'GHCND:US1NJBG0003',\n",
    "  60206: 'GHCND:USW00094789',\n",
    "  60207: 'GHCND:USW00094789',\n",
    "  60405: 'GHCND:USW00094789',\n",
    "  60406: 'GHCND:USW00094789',\n",
    "  60416: 'GHCND:US1NJBG0003',\n",
    "  60605: 'GHCND:USW00094789',\n",
    "  60606: 'GHCND:USW00094789',\n",
    "  60808: 'GHCND:USW00094789',\n",
    "  60809: 'GHCND:USW00094789',\n",
    "  60819: 'GHCND:US1NJBG0003',\n",
    "  61011: 'GHCND:USW00094789',\n",
    "  61012: 'GHCND:USW00094789',\n",
    "  61024: 'GHCND:US1NJBG0003',\n",
    "  61238: 'GHCND:USW00094789',\n",
    "  61239: 'GHCND:USW00094789',\n",
    "  61451: 'GHCND:USW00094789',\n",
    "  61452: 'GHCND:USW00094789',\n",
    "  61632: 'GHCND:USW00094789',\n",
    "  61633: 'GHCND:USW00094789',\n",
    "  61822: 'GHCND:USW00094789',\n",
    "  61823: 'GHCND:USW00094789',\n",
    "  62022: 'GHCND:USW00094789',\n",
    "  62023: 'GHCND:USW00094789',\n",
    "  62214: 'GHCND:USW00094789',\n",
    "  62215: 'GHCND:USW00094789',\n",
    "  62226: 'GHCND:US1NJBG0003',\n",
    "  62423: 'GHCND:USW00094789',\n",
    "  62424: 'GHCND:USW00094789',\n",
    "  62434: 'GHCND:US1NJBG0003',\n",
    "  62626: 'GHCND:USW00094789',\n",
    "  62627: 'GHCND:USW00094789',\n",
    "  62637: 'GHCND:US1NJBG0003',\n",
    "  62848: 'GHCND:USW00094789',\n",
    "  62849: 'GHCND:USW00094789',\n",
    "  63031: 'GHCND:USW00094789',\n",
    "  63032: 'GHCND:USW00094789',\n",
    "  63221: 'GHCND:USW00094789',\n",
    "  63222: 'GHCND:USW00094789',\n",
    "  63233: 'GHCND:US1NJBG0003',\n",
    "  63440: 'GHCND:USW00094789',\n",
    "  63441: 'GHCND:USW00094789',\n",
    "  63451: 'GHCND:US1NJBG0003',\n",
    "  63665: 'GHCND:USW00094789',\n",
    "  63666: 'GHCND:USW00094789',\n",
    "  63676: 'GHCND:US1NJBG0003',\n",
    "  63871: 'GHCND:USW00094789',\n",
    "  63872: 'GHCND:USW00094789',\n",
    "  63882: 'GHCND:US1NJBG0003',\n",
    "  64092: 'GHCND:USW00094789',\n",
    "  64093: 'GHCND:USW00094789',\n",
    "  64103: 'GHCND:US1NJBG0003',\n",
    "  64319: 'GHCND:USW00094789',\n",
    "  64320: 'GHCND:USW00094789',\n",
    "  64514: 'GHCND:USW00094789',\n",
    "  64515: 'GHCND:USW00094789',\n",
    "  64715: 'GHCND:USW00094789',\n",
    "  64716: 'GHCND:USW00094789',\n",
    "  64923: 'GHCND:USW00094789',\n",
    "  64924: 'GHCND:USW00094789',\n",
    "  64934: 'GHCND:US1NJBG0003',\n",
    "  65128: 'GHCND:USW00094789',\n",
    "  65129: 'GHCND:USW00094789',\n",
    "  65139: 'GHCND:US1NJBG0003',\n",
    "  65350: 'GHCND:USW00094789',\n",
    "  65351: 'GHCND:USW00094789'},\n",
    " 'attributes': {58914: ',,N,',\n",
    "  59144: ',,W,',\n",
    "  59145: ',,W,2400',\n",
    "  59155: ',,N,',\n",
    "  59372: ',,W,',\n",
    "  59373: ',,W,2400',\n",
    "  59580: ',,W,',\n",
    "  59581: ',,W,2400',\n",
    "  59592: ',,N,',\n",
    "  59802: ',,W,',\n",
    "  59803: ',,W,2400',\n",
    "  59996: ',,W,',\n",
    "  59997: ',,W,2400',\n",
    "  60007: ',,N,',\n",
    "  60206: ',,W,',\n",
    "  60207: ',,W,2400',\n",
    "  60405: ',,W,',\n",
    "  60406: ',,W,2400',\n",
    "  60416: ',,N,',\n",
    "  60605: ',,W,',\n",
    "  60606: ',,W,2400',\n",
    "  60808: ',,W,',\n",
    "  60809: ',,W,2400',\n",
    "  60819: ',,N,',\n",
    "  61011: ',,W,',\n",
    "  61012: ',,W,2400',\n",
    "  61024: ',,N,',\n",
    "  61238: ',,W,',\n",
    "  61239: ',,W,2400',\n",
    "  61451: ',,W,',\n",
    "  61452: ',,W,2400',\n",
    "  61632: ',,W,',\n",
    "  61633: ',,W,2400',\n",
    "  61822: ',,W,',\n",
    "  61823: ',,W,2400',\n",
    "  62022: ',,W,',\n",
    "  62023: ',,W,2400',\n",
    "  62214: ',,W,',\n",
    "  62215: ',,W,2400',\n",
    "  62226: ',,N,',\n",
    "  62423: ',,W,',\n",
    "  62424: ',,W,2400',\n",
    "  62434: ',,N,',\n",
    "  62626: ',,W,',\n",
    "  62627: ',,W,2400',\n",
    "  62637: ',,N,',\n",
    "  62848: ',,W,',\n",
    "  62849: ',,W,2400',\n",
    "  63031: ',,W,',\n",
    "  63032: ',,W,2400',\n",
    "  63221: ',,W,',\n",
    "  63222: ',,W,2400',\n",
    "  63233: ',,N,',\n",
    "  63440: ',,W,',\n",
    "  63441: ',,W,2400',\n",
    "  63451: ',,N,',\n",
    "  63665: ',,W,',\n",
    "  63666: ',,W,2400',\n",
    "  63676: ',,N,',\n",
    "  63871: ',,W,',\n",
    "  63872: ',,W,2400',\n",
    "  63882: ',,N,',\n",
    "  64092: ',,W,',\n",
    "  64093: ',,W,2400',\n",
    "  64103: ',,N,',\n",
    "  64319: ',,W,',\n",
    "  64320: ',,W,2400',\n",
    "  64514: ',,W,',\n",
    "  64515: ',,W,2400',\n",
    "  64715: ',,W,',\n",
    "  64716: ',,W,2400',\n",
    "  64923: ',,W,',\n",
    "  64924: ',,W,2400',\n",
    "  64934: ',,N,',\n",
    "  65128: ',,W,',\n",
    "  65129: ',,W,2400',\n",
    "  65139: ',,N,',\n",
    "  65350: ',,W,',\n",
    "  65351: ',,W,2400'},\n",
    " 'value': {58914: 0.0,\n",
    "  59144: 0.0,\n",
    "  59145: 0.0,\n",
    "  59155: 0.0,\n",
    "  59372: 0.0,\n",
    "  59373: 0.0,\n",
    "  59580: 0.0,\n",
    "  59581: 0.0,\n",
    "  59592: 0.0,\n",
    "  59802: 0.0,\n",
    "  59803: 0.0,\n",
    "  59996: 0.0,\n",
    "  59997: 0.0,\n",
    "  60007: 0.0,\n",
    "  60206: 0.0,\n",
    "  60207: 0.0,\n",
    "  60405: 0.0,\n",
    "  60406: 0.0,\n",
    "  60416: 0.0,\n",
    "  60605: 0.0,\n",
    "  60606: 0.0,\n",
    "  60808: 0.0,\n",
    "  60809: 0.0,\n",
    "  60819: 0.0,\n",
    "  61011: 0.0,\n",
    "  61012: 0.0,\n",
    "  61024: 0.0,\n",
    "  61238: 0.0,\n",
    "  61239: 0.0,\n",
    "  61451: 0.0,\n",
    "  61452: 0.0,\n",
    "  61632: 0.0,\n",
    "  61633: 0.0,\n",
    "  61822: 0.0,\n",
    "  61823: 0.0,\n",
    "  62022: 0.0,\n",
    "  62023: 0.0,\n",
    "  62214: 0.0,\n",
    "  62215: 0.0,\n",
    "  62226: 0.0,\n",
    "  62423: 0.0,\n",
    "  62424: 0.0,\n",
    "  62434: 0.0,\n",
    "  62626: 0.0,\n",
    "  62627: 0.0,\n",
    "  62637: 0.0,\n",
    "  62848: 0.0,\n",
    "  62849: 0.0,\n",
    "  63031: 0.0,\n",
    "  63032: 0.0,\n",
    "  63221: 0.0,\n",
    "  63222: 0.0,\n",
    "  63233: 0.0,\n",
    "  63440: 0.0,\n",
    "  63441: 0.0,\n",
    "  63451: 0.0,\n",
    "  63665: 0.0,\n",
    "  63666: 0.0,\n",
    "  63676: 0.0,\n",
    "  63871: 0.0,\n",
    "  63872: 0.0,\n",
    "  63882: 0.0,\n",
    "  64092: 0.0,\n",
    "  64093: 0.0,\n",
    "  64103: 0.0,\n",
    "  64319: 0.0,\n",
    "  64320: 0.0,\n",
    "  64514: 0.0,\n",
    "  64515: 0.0,\n",
    "  64715: 0.0,\n",
    "  64716: 0.0,\n",
    "  64923: 0.0,\n",
    "  64924: 0.0,\n",
    "  64934: 0.0,\n",
    "  65128: 0.0,\n",
    "  65129: 0.0,\n",
    "  65139: 0.0,\n",
    "  65350: 0.0,\n",
    "  65351: 0.0}\n",
    " }\n",
    "                \n",
    "df = mk_dataframe('weather')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 DateTime Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_datetime_dataframe():\n",
    "  datetime_df = pd.DataFrame(\n",
    "    {'Date': {0: ('2020-03-13 20:00:00'),\n",
    "      1: ('2020-03-13 19:00:00'),\n",
    "      2: ('2020-03-13 18:00:00'),\n",
    "      3: ('2020-03-13 17:00:00'),\n",
    "      4: ('2020-03-13 16:00:00'),\n",
    "      5: ('2020-03-13 15:00:00')},\n",
    "    'Symbol': {0: 'ETHUSD',\n",
    "      1: 'ETHUSD',\n",
    "      2: 'ETHUSD',\n",
    "      3: 'ETHUSD',\n",
    "      4: 'ETHUSD',\n",
    "      5: 'ETHUSD'},\n",
    "    'Open': {0: 129.94, 1: 119.51, 2: 124.47, 3: 124.08, 4: 124.85, 5: 128.39},\n",
    "    'High': {0: 131.82, 1: 132.02, 2: 124.85, 3: 127.42, 4: 129.51, 5: 128.9},\n",
    "    'Low': {0: 126.87, 1: 117.1, 2: 115.5, 3: 121.63, 4: 120.17, 5: 116.06},\n",
    "    'Close': {0: 128.71, 1: 129.94, 2: 119.51, 3: 124.47, 4: 124.08, 5: 124.85},\n",
    "    'Volume': {0: 1940673.93,\n",
    "      1: 7579741.09,\n",
    "      2: 4898735.81,\n",
    "      3: 2753450.92,\n",
    "      4: 4461424.71,\n",
    "      5: 7378976.0}}\n",
    "  )\n",
    "  datetime_df['Date'] = pd.to_datetime(datetime_df['Date'])\n",
    "  datetime_df.set_index('Date', inplace=True)       # Setting date column as an index for later functions\n",
    "  datetime_df.index\n",
    "  return datetime_df\n",
    "\n",
    "datetime_df = mk_datetime_dataframe()\n",
    "# datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Making a Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'people' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kthompso\\OneDrive - CAIRNS REGIONAL COUNCIL\\04-Git\\09-Hands-On-Data-Analysis-with-Pandas-2nd-edition\\Cheat_Sheet-V02.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39mDataFrame(people)                  \u001b[39m# Making a dataframe from a dictionary\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=1'>2</a>\u001b[0m \u001b[39m# df2 = pd.DataFrame(people2)                 # These are used on Corry's examples later\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=2'>3</a>\u001b[0m \u001b[39m# dirty_df = mk_dataframe('dirty')            # These are used on Corry's examples later\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=3'>4</a>\u001b[0m \u001b[39m# Load csv in here? bad_df = pd.DataFrame(bad_data)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=4'>5</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=5'>6</a>\u001b[0m \u001b[39m# Add a whole new dataframe as new rows\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kthompso/OneDrive%20-%20CAIRNS%20REGIONAL%20COUNCIL/04-Git/09-Hands-On-Data-Analysis-with-Pandas-2nd-edition/Cheat_Sheet-V02.ipynb#ch0000014?line=6'>7</a>\u001b[0m pd\u001b[39m.\u001b[39mconcat([df, aggregates_df], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'people' is not defined"
     ]
    }
   ],
   "source": [
    "df =  pd.DataFrame(people)                  # Making a dataframe from a dictionary\n",
    "# df2 = pd.DataFrame(people2)                 # These are used on Corry's examples later\n",
    "# dirty_df = mk_dataframe('dirty')            # These are used on Corry's examples later\n",
    "# Load csv in here? bad_df = pd.DataFrame(bad_data)\n",
    "\n",
    "# Add a whole new dataframe as new rows\n",
    "pd.concat([df, aggregates_df], axis=1)      # Merges the 2 dataframes alomng the column (#1) axis \n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Overview of the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()             # Overview of the dataframe\n",
    "# df.columns            # List column names\n",
    "# df.head(10)           # List top x rows (default is 5)\n",
    "# df.tail()             # List bottom x rows (default is 5)\n",
    "# df.sample()           # List randon x rows (default is 1)\n",
    "# df.describe()         # Quick summart of the frame, best for wide format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Set a new index. Keep it set with `inplace``.  \n",
    "# Indexes don't have to be unique\n",
    "df.set_index('email', inplace=True)     # Set a column to be an index\n",
    "print(df.index)\n",
    "print(df)\n",
    "\n",
    "df.reset_index(inplace=True)            # Reset row indexes to (hand to 'save'a column used a an index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accessing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Access Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "df                                # Simple access\n",
    "# df['email']                       # Access single column\n",
    "# df[['last', 'email']]             # Access multiple columns by using a list (a list within the list)i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Access Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "df.set_index('email', inplace=True)     #Setup\n",
    "# df.iloc[[0, 1], 2]                    # Access by integer reference / index by using .iloc.  \n",
    "                                        # .loc and iloc takes row index first\n",
    "\n",
    "# df.loc[                               # Access by row index name .loc\n",
    "#     'CoreyMSchafer@gmail.com', 'last']   \n",
    "\n",
    "# df.loc[                               # As above plus multi selected rows and columns \n",
    "#     ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com'], \n",
    "#     ['first', 'last']]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best to filter with 2 part process:\n",
    "1. Set filter \n",
    "2. Apply filter\n",
    "\n",
    "_But can't use word \"filter\" as a variable name it's reserved_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "df.set_index('email', inplace=True)     #Setup\n",
    "\n",
    "filt = (df['last'] == 'Schafer') |(   # 1) Set filter.  An exampe of an 'or' '|' filter\n",
    "    df['first'] == 'John') \n",
    "df.loc[filt, 'last']                  # 2) Apply filter or\n",
    "# df.loc[~filt, 'last']                 # 2) Apply inverse of filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Querying XXX WIP XXX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Qurying Data\n",
    "# snow_data = weather.query('datatype == \"SNOW\" and value > 0 and station.str.contains(\"US1NY\")')\n",
    "# snow_data.head()\n",
    "\n",
    "# valid_station = dirty_data.query('station != \"?\"').drop(columns=['WESF', 'station'])\n",
    "# station_with_wesf = dirty_data.query('station == \"?\"').drop(columns=['station', 'TOBS', 'TMIN', 'TMAX'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Checking for Dirty Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('dirty')\n",
    "# Checking for Nulls\n",
    "# df.info()                 # Will show missing values (nulls) and data types\n",
    "# dirty_df.isna().sum()     # Identify na values (by getting a mask) rather than drop them with .isna\n",
    "# # or\n",
    "# dirty_df.isna()\n",
    "\n",
    "# Checking for wrong Types\n",
    "# dirty_df.dtypes           # Identify if data type is correct. \n",
    "                            # If numeric are wrong many aggrate functions won't work \n",
    "\n",
    "# df.describe()             # This will show some errors up in the dataset, \n",
    "                            # eg unreasonably large or small\n",
    "\n",
    "# df.describe(              # Check the describe for datetime and others\n",
    "#     include='object')  \n",
    "\n",
    "# df[df.duplicated(          # Returns the rows (after the first) that\n",
    "#     ['first', 'last'])]    # are duplicated in the columns mentioned   \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dropping Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty_df.dropna()                                 # Drop any / all _rows_ that aren't totally complete with .dropna & how = 'any'\n",
    "                                                    # default values are: dirty_df.dropna(axis='index', how='any')\n",
    "\n",
    "# dirty_df.dropna(                                  # Drop rows that have missing data in 'any' specified rows with subset=[]\n",
    "    # axis='index', how='any', \n",
    "    # subset=['last', 'email'])\n",
    "\n",
    "# dirty_df.dropna(axis='columns')                   # Drop incomplete _columns_.  Which is all of them due to row 4\n",
    "\n",
    "# dirty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Replacing Dirty Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Replacing Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Nulls\n",
    "dirty_df.replace('NA', np.nan, inplace=True)          # Replace unusual 'nill' values (in these cases 'NA' & 'Missing') \n",
    "dirty_df.replace('Missing', np.nan, inplace=True)     # with the proper np.nan value across whole data frame\n",
    "# Could do all this at import time for csv pd.read_csv(XXXXX..., na_values=['NA','None'])\n",
    "\n",
    "dirty_df.fillna(0)                                    # Replaces np.nan  values with an actual value. Most usful for NUMERIC data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Replacing Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Bad Types\n",
    "# dirty_df['age'] = df['age'].astype(float)             # Casting a column to the correct data type with .astype\n",
    "                                                        # Can use .astype on whole dataframe too.\n",
    "                                                        # Use float not int, as NaN is a float.\n",
    "# More elegant way to change multiple types:\n",
    "# !!!!! Needs example updating as data values don't marry up !!!!!\n",
    "# df = df.assign(\n",
    "#     date=       lambda x: pd.to_datetime(x['date']),\n",
    "#     volume =    lambda x: x['volume'].astype(int)\n",
    "#     )\n",
    "                                                        \n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Updating Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Update Column Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "# df.set_index('email', inplace=True)     #Setup\n",
    "\n",
    "# df.columns = ['email', 'first_name', 'last_name']         # Rename all columns \n",
    "\n",
    "# df.rename(                                                # Rename specific columns using .rename\n",
    "# df.set_index('email', inplace=True)     #Setup\n",
    "#     columns={\n",
    "#         'first_name': 'first', 'last_name': 'last'\n",
    "#         }, inplace=True                                   # Note, need \"inplace\" \n",
    "#     ) \n",
    " \n",
    "# df.columns = [x.upper() for x in df.columns]              # Rename all columns by an inline comprehension .columns\n",
    "\n",
    "# Reset\n",
    "df.columns = [x.lower() for x in df.columns]                # Reset so later examples work\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Update Values - Direct Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email'] = df['email'].str.lower()                               # Update whole column with string object method with.str.x\n",
    "df.loc[3] = ['John2Smith@email.com', 'John2', 'Smith']              # Update whole row with .loc\n",
    "df.loc[2, ['last', 'email']] = ['Smith', 'janeysmith@email.com']    # Update specific columns of a row with .loc\n",
    "\n",
    "# Update based on filter \n",
    "filt = (df['email'] == 'John2Smith@email.com')                      # Update cells based on a filter with .loc\n",
    "# df[filt]['last'] = 'Smith'                                        # DON'T do this, it won't work\n",
    "df.loc[filt, 'first'] = 'Johnny'                                    # THIS will, need .loc\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Updating Values - with Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four Functions:\n",
    "- `apply`\n",
    "- `applymap` \n",
    "- `map`\n",
    "- `replace`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 `apply` a function to an object (dataframe or series) and get a series as a result\n",
    "- Object can be a series (by default a column) \n",
    "- Object can be a dataframe in which case it's applied to each series (column) for a single result for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying to a column\n",
    "# df['email'].apply(len)            # `apply` the `len` function to the email column\n",
    "\n",
    "# def update_email(email):          # 'apply' your own function\n",
    "#     return email.upper()\n",
    "# df['email'].apply(update_email) \n",
    "\n",
    "# df['email'].apply(                # 'Apply' a your own inline (LAMBDA) function \n",
    "#     lambda x: x.lower()           # to a whole column and get a series as a result\n",
    "#     )  \n",
    "\n",
    "# When applied to a dataframe 'apply' is applied across each series\n",
    "df.apply(len) # or df.apply(len, axis='columns') or df.apply(len, axis='rows')   \n",
    "# df.apply(pd.Series.min)           # Returns the minimum (first in alaphs) in each column\n",
    "\n",
    "# df.apply(                           # Applying a Lambda function to each series\n",
    "#     lambda x: x.min()\n",
    "#     )     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Applying Functions\n",
    "# We can use the `apply()` method to run the same operation \n",
    "# oct_weather_z_scores = central_park_weather\\\n",
    "#     .loc['2017-10', ['TMIN', 'TMAX', 'PRCP']]\\\n",
    "#     .apply(lambda x: x.sub(x.mean()).div(x.std()))\n",
    "# oct_weather_z_scores.describe().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 `applymap` a function to a dataframe and get a dataframe as a result.  \n",
    "Applied elementwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.applymap(len)\n",
    "df.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3 `map` a series and get a series as a result.  \n",
    "Replaces __all__ elements in series  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .map only works on a series. Use like a vlookup\n",
    "# Use it to subsitute one value for another via a lookup dictionary.\n",
    "# Unsubtituted vales replaced by NaN\n",
    "df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.4 `replace` on an object (series or dataframe) a get same object as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .replace works like map but leaves unsubsittuted values untouched (not NaN)\n",
    "df['first'] = df['first'].replace({'Corey': 'Corey2', 'Jane': 'Jane2'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Arithmetic & Statistic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arithmetic and statistics\n",
    "# We already saw that we can use mathematical operators like `+` and `/` with dataframes directly. However, we can also use methods, which allow us to specify the axis to perform the calculation over. By default, this is per column. Let's find the Z-scores for the volume traded and look at the days where this was more than 2 standard deviations from the mean:\n",
    "\n",
    "# .sub\n",
    "# .mean()\n",
    "# .div\n",
    "# .std()\n",
    "# .abs()\n",
    "# .rank()\n",
    "# .pct_change()\n",
    "# .any()\n",
    "# .all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXXX WIP XXXX\n",
    "# fb.assign(\n",
    "#     abs_z_score_volume=lambda x: \\\n",
    "#         x['volume'].sub(x['volume'].mean()).div(x['volume'].std()).abs()\n",
    "# ).query('abs_z_score_volume > 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Updating Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Can't use . notation as pandas would look for method\n",
    "\n",
    "# Split data with str.split.  Splits on space by default so not needed\n",
    "# would give list by default, need expand=True to make 2 new columns in dataframe\n",
    "# df['full_name'].str.split(' ', expand=True)\n",
    "\n",
    "# Creating a new column with strings, can use numeric as well with .apply \n",
    "df['full_name'] = df['first'] + ' ' + df['last']\n",
    "\n",
    "# Create multiple columns at once \n",
    "# df[['first', 'last']] = df['full_name'].str.split(' ', expand=True)\n",
    "\n",
    "# Add new columns\n",
    "df['numeric_data_01'] = \\\n",
    "    np.random.randint(0,100, size=len(df))           # These one is needed for the aggregate examples later\n",
    "df['numeric_data_02'] = \\\n",
    "    np.random.randint(0,100, size=len(df))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "df                                # Simple access\n",
    "# df['email']                       # Access single column\n",
    "# df[['last', 'email']]             # Access multiple columns by using a list (a list within the list)i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with .drop like a db\n",
    "df.drop(columns=['first', 'last'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Adding Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Adding a single row with .append (Now deprecated)\n",
    "# df.append({'first': 'Tony'}, ignore_index=True) # insert new row even if no index given: ignore_index=True\n",
    "\n",
    "# So use:\n",
    "# df2 = pd.DataFrame({'first': ['Tony']})\n",
    "# pd.concat([df, df2])\n",
    "\n",
    "\n",
    "# Add a whole new dataframe as new rows\n",
    "# Set-Up New dataframe\n",
    "# aggregates_df = pd.DataFrame()\n",
    "# aggregates_df['numeric_data_01'] = \\\n",
    "#     np.random.randint(0,100, size=len(df))  \n",
    "# aggregates_df['numeric_data_02'] = \\\n",
    "#     np.random.randint(0,100, size=len(df))\n",
    "# pd.concat([df, aggregates_df], axis=1)      # Merges the 2 dataframes alomng the column (#1) axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Dropping Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(index=3, inplace=True)                # Deleteing a row with .drop\n",
    "\n",
    "filt = df['full_name'] == 'Jane2 Doe'         # Dropping rows based on values.  This case index\n",
    "# df.drop(index=df[filt].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows based on values \n",
    "# filt = df['last'] == 'Stark'\n",
    "# df.drop(index=df[filt].index)\n",
    "# df.drop(index=df[filt].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1 Concatanating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1 = mk_dataframe('people')\n",
    "# Adding a whole new dataframe as new rows\n",
    "\n",
    "pd.concat([df, df1],         ignore_index=True, sort=False) # Adds as rows \n",
    "pd.concat([df, df1], axis=1, ignore_index=True, sort=False) # Adds as columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2 Merging (on any Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up\n",
    "station_info = mk_dataframe('stations')\n",
    "weather = mk_dataframe('weather')\n",
    "pd.to_datetime(weather['date'])\n",
    "weather.set_index('date' , inplace= True)\n",
    "weather['overlap_cl'] = 'weather'\n",
    "station_info['overlap_cl'] = 'station'\n",
    "##########################################\n",
    "# Merging Dataframes\n",
    "# By default, `merge()` performs an inner join. \n",
    "# We simply specify the columns to use for the join. \n",
    "# The left dataframe is the one we call `merge()` on, and the right one is passed in as an argument:\n",
    "\n",
    "inner_join = weather.merge(station_info, left_on='station', right_on='id')\n",
    "left_join = station_info.merge(weather, left_on='id', right_on='station', how='left')\n",
    "right_join = weather.merge(station_info, left_on='station', right_on='id', how='right', suffixes=('_l', '_r'))\n",
    "\n",
    "# valid_station.merge(\n",
    "#     station_with_wesf, how='left', left_index=True, right_index=True, suffixes=('', '_?')\n",
    "# ).query('WESF > 0').head()\n",
    "\n",
    "# inner_join\n",
    "# left_join\n",
    "right_join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3 Joining (on Index only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up\n",
    "station_info = mk_dataframe('stations')\n",
    "weather = mk_dataframe('weather')\n",
    "pd.to_datetime(weather['date'])\n",
    "weather.set_index('date' , inplace= True)\n",
    "weather['overlap_cl'] = 'weather'\n",
    "station_info['overlap_cl'] = 'station'\n",
    "##########################################\n",
    "# Merge will do everything that .join can do. \n",
    "# but .join is a bit easier to use but only works on indexes\n",
    "\n",
    "\n",
    "weather.index.intersection(station_info.index)\n",
    "# weather.index.difference(station_info.index)\n",
    "# station_info.index.difference(weather.index)\n",
    "# weather.index.unique().union(station_info.index)\n",
    "\n",
    "# valid_station.join(station_with_wesf, how='left', rsuffix='_?').query('WESF > 0').head() Joins can be very resource-intensive, so it's a good idea to figure out what type of join you need using set operations before trying the join itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Sort a Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email'].sort_values()    # Sort a series (column) with .sort_values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Sort a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by='email', ascending=False)   # Sort a dataframe by a single column with sort_values\n",
    "\n",
    "df.sort_values(                                 # Sort a dataframe by a multiple columnsin a list with .sort_values\n",
    "    by=['email', 'full_name'], \n",
    "    ascending=False)  \n",
    "\n",
    "# df.sort_values(                               # Sort a dataframe by a multiple columns in a list with .sort_values \n",
    "#     by=['email', 'full_name'],                # and different asending attrbutes from a list and make perm with inpace \n",
    "#     ascending=[False, True], \n",
    "#     inplace=True  \n",
    "#     )\n",
    "\n",
    "df.sort_index()                               # Reset the order based on the \"original\" index with .sort_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates_df[[                                     # Use aggregation functuins, such as:\n",
    "#     'numeric_data_01', 'numeric_data_02']].median() # mean, mode, standard deviation on a simgle column\n",
    "\n",
    "# aggregates_df['numeric_data_01'].count()            # count the number of populated fields in a column with .count\n",
    "\n",
    "# aggregates_df['numeric_data_01'].value_counts()     # count the number of eachvalue with .value_counts \n",
    "\n",
    "# aggregates_df['numeric_data_01'].value_counts(        # or to get a percentage use the normalise=True attribute\n",
    "#     normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Groups, Bins and Windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a group in a similar way as we created a filter, but with .groupby([column_name])\n",
    "# This gives you a group object, indexed by the group rather than true / galse list of a filter\n",
    "grp_last = df.groupby(['last'])\n",
    "grp_last.groups                # KT added to see groups and indexes\n",
    "\n",
    "# Then apply methods to the group in a 2nd step, e.g., .get_group \n",
    "grp_last.get_group('Doe')\n",
    "\n",
    "# Apply a function (.value_counts) to a column after already being grouped\n",
    "# Can filter furtther with .loc makes it loke usiong a filter\n",
    "# Can also get percentage like above with (normalize=True)*100\n",
    "grp_last['first'].value_counts() #.loc['Smith']\n",
    "\n",
    "# Can retrive multiple columns and perform other aggregate functions with their methods \n",
    "grp_last[['numeric_data_01', 'numeric_data_02']].median() #.loc[['Smith' , 'Doe']]\n",
    "\n",
    "# *** Or use more generic form to apply multiple aggregated functions with .agg ***\n",
    "# Seems most generic to me!!!\n",
    "grp_last[['numeric_data_01', 'numeric_data_02']].agg(['count', 'mean', 'std']) #.loc[['Smith' , 'Doe']]\n",
    "\n",
    "# Counting rows with filter.  Counts true's in the returned series with .sum\n",
    "filt = df['last'] == 'Doe'\n",
    "df.loc[filt]['first'].str.contains('Jane').sum()\n",
    "\n",
    "# But for a group need to .apply the function to all the group's series \n",
    "grp_last['first'].apply(lambda x: x.str.contains('n').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Percentages with Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Percentage With Groups\n",
    "\n",
    "# s How to find the percentage with an n in their first name and group by surname\n",
    "\n",
    "# Create a series of the number of people with each surname\n",
    "surname_count = df['last'].value_counts()\n",
    "surname_count\n",
    "\n",
    "# Create a series of people with each surname, with 'n' in first name\n",
    "surname_count_with_n = grp_last['first'].apply(lambda x: x.str.contains('n').sum())\n",
    "surname_count_with_n\n",
    "\n",
    "# Merge the 2 series togther, add and calculate the percentage (answer column) and tidy up column names\n",
    "df_with_n = pd.concat([surname_count, surname_count_with_n], axis='columns', sort=False)\n",
    "df_with_n['percentage'] = df_with_n['first']/df_with_n['last']*100\n",
    "df_with_n.rename(columns={'first': 'First_with_an_n', 'last': 'Surname'}, inplace=True)\n",
    "df_with_n.sort_values('percentage', ascending=False)\n",
    "# df_with_n.loc['Smith']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  XXX WIP XXX\n",
    "## Bins\n",
    "# pd.cut() to create bins of even range in volume\n",
    "# pd.qcut() to create bins of even content counts\n",
    "\n",
    "# volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "# volume_binned.value_counts()\n",
    "\n",
    "# volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "# volume_binned.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows \n",
    "# .rolling method with .assign for new columns \n",
    "# central_park_weather.loc['2018-10'].assign(\n",
    "#     rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()\n",
    "##     and other in here too \n",
    "# )[['PRCP', 'rolling_PRCP']].head(7).T\n",
    "\n",
    "# Whole dataframe at once or 'apply' to whole dataframe at onces\n",
    "# central_park_weather.loc['2018-10'].rolling('3D').mean().iloc[:7,:6]\n",
    "\n",
    "# Use .agg for different agg methgods\n",
    "# central_park_weather['2018-10-01':'2018-10-07'].rolling('3D').agg(\n",
    "#     {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'}\n",
    "# ).join( # join with original data for comparison\n",
    "#     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "#     lsuffix='_rolling'\n",
    "# ).sort_index(axis=1) # sort columns so rolling calcs are next to originals\n",
    "\n",
    "# ewm() method for exponentially weighted moving calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .expanding gives and expaning window rather then a rolling one\n",
    "# equivalent to cumulative aggregations like `cumsum()` however\n",
    "# - we aren't limited to predefined aggregations. \n",
    "# - can specify the minimum number of periods required to start calculating\n",
    "    \n",
    "# central_park_weather.loc['2018-06'].assign(\n",
    "#     TOTAL_PRCP=lambda x: x.PRCP.cumsum(),\n",
    "#     AVG_PRCP=lambda x: x.PRCP.expanding().mean()\n",
    "# ).head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T \n",
    "\n",
    "# central_park_weather['2018-10-01':'2018-10-07'].expanding().agg(\n",
    "#     {'TMAX': np.max, 'TMIN': np.min, 'AWND': np.mean, 'PRCP': np.sum}\n",
    "# ).join(\n",
    "#     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "#     lsuffix='_expanding'\n",
    "# ).sort_index(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Making a Column Datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_df['Date'] = pd.to_datetime(datetime_df['Date'])\n",
    "# datetime_df.set_index(['Date'], inplace=True)\n",
    "# datetime_df.dtypes\n",
    "# datetime_df\n",
    "\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df['Date'] = \\\n",
    "    pd.to_datetime(datetime_df['Date'])     # Apply the pandas to_datetime function to a column             \n",
    "\n",
    "# datetime_df['Date'] = \\\n",
    "#     datetime_df['Date'].apply(pd.to_datetime)# Same as above\n",
    "\n",
    "# Can do at import time if prefered\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Using Datetime Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_df.loc[0, 'Date'].day_name()   # To find the day name of a single datetime\n",
    "\n",
    "# datetime_df['DayOfWeek'] =\\\n",
    "#     datetime_df['Date'].dt.day_name()   # New column comtaining day name with .dt.day_name()\n",
    "\n",
    "# Some self explanatory date functions\n",
    "# print(datetime_df['Date'].min())\n",
    "# print(datetime_df['Date'].max())\n",
    "# print(datetime_df['Date'].max() - datetime_df['Date'].min()) # Known as time delta\n",
    "\n",
    "# Filtering on date range in str converted to a datetime with .to_datetime\n",
    "filt = (\n",
    "    datetime_df['Date'] >= pd.to_datetime('2020-03-13 16:00:00')) & (\n",
    "    datetime_df['Date'] < pd.to_datetime('2020-03-13 18:00:00'))\n",
    "\n",
    "datetime_df.loc[filt]\n",
    "# datetime_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Using Datetime as an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.set_index('Date', inplace=True)       # Setting date column as an index for later functions\n",
    "datetime_df.index\n",
    "\n",
    "datetime_df.loc['2020-03-13 16:00']               # Single value slice on index with .loc\n",
    "\n",
    "# datetime_df.loc[                                  # Slice on index with .loc and for range :\n",
    "#     '2020-03-13 17:00':'2020-03-13 19:00']                    \n",
    "\n",
    "# datetime_df.loc[\n",
    "#     '2020-03-13 17:00':'2020-03-13 19:00'][        # Get an aggregate value of a column sliced by date \n",
    "#     'Close'].mean()    \n",
    "\n",
    "# datetime_df['High'].resample('D').max()         # Resample (downsample) a range using 'D' for day and .resample\n",
    "# datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample whole dataframe with single aggregation method\n",
    "df.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample whole dataframe with diferent aggregations with a map & .agg method\n",
    "df.resample('W').agg({'Close': 'mean', 'High': 'max', 'Low': 'min', 'Volume': 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df = mk_datetime_dataframe()\n",
    "\n",
    "# Setup Series to Plot\n",
    "highs = datetime_df['High'].resample('H').max()\n",
    "highs\n",
    "\n",
    "# # Quick line plot with mathplot & a Magic command needed for Jupyter notebook\n",
    "%matplotlib inline \n",
    "highs.plot()\n",
    "\n",
    "# datetime_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Common Problems  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.1 \"SettingWithCopyWarning\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"SettingWithCopyWarning\" fixed with .copy() \n",
    "# to explicitly decare that new opbject is a copy not a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX.  To File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({      # Really nice way to gather summary stats for targeted rows into a data frame\n",
    "#     'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n",
    "#     '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n",
    "# }).T                # Note the .T to transpose the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need somthing on np.vectorize() to vectorize functions similar to how `map()` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipes = 'put result into fillowing function'\n",
    "# We can use pipes to apply any function that accepts our data as the first argument and pass in any additional arguments. This makes it easy to chain steps together regardless of whether they are methods or functions:\n",
    "\n",
    "# f(g(h(data), 20), x=True)\n",
    "# = same as =\n",
    "# data.pipe(h)\\\n",
    "#     .pipe(g, 20)\\\n",
    "#     .pipe(f, x=True)\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b9aa4b47fde6b9659f2be704d3beb3fd8a605c3f7f9db8b3f63f09d360b7471"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('conda_3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

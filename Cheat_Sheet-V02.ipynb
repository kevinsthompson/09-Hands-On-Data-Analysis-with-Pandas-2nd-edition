{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kevin's Pandas' Crib Sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whays to get help:\n",
    "\n",
    "| Command           | Function |\n",
    "|---------------    |----------|\n",
    "| Auto complete     | To get options |\n",
    "| Hover             |     To get docstring |\n",
    "| `dir(func)`       | Shows all method and function calls |\n",
    "| `help(func)`      | See documentation |\n",
    "| ?                 | Access documentation |\n",
    "| ??                | Access source code |\n",
    "| %lsmagic          | List available magic commands |\n",
    "| %quickref         | Magic quick refernce sheet |\n",
    "\n",
    "Good guide [here](https://problemsolvingwithpython.com/02-Jupyter-Notebooks/02.07-Getting-Help-in-a-Jupyter-Notebook/)\n",
    "\n",
    "Good tips, but focused on Jupyter Notebook in browser [here](https://towardsdatascience.com/15-tips-and-tricks-for-jupyter-notebook-that-will-ease-your-coding-experience-e469207ac95c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a consolidation of notes and examples from:\n",
    "> Coreys MSchafer's Pandas videos [here](https://www.youtube.com/playlist?list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS) \n",
    "\n",
    "and \n",
    "\n",
    "> Hands on Data Analysis by Stefanie Molin\n",
    "All data in examples and exercises available [here](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition)\n",
    "\n",
    "Version 2.1W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set-Up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Main Dataset Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Functions that create example datasets for use later \n",
    "\n",
    "def mk_dictionary(x):\n",
    "    if x == \"people\":\n",
    "        dictionary = {\n",
    "            'first': ['Corey', 'Jane', 'Janey', 'John', 'Jimmy'], \n",
    "            'last': ['Schafer', 'Doe', 'Doe', 'Doe', 'Doe'], \n",
    "            'email': [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JaneyDoe@email.com','JohnDoe@email.com', 'JimmyDoe@email.com']\n",
    "        }\n",
    "    elif x == 'people2':\n",
    "        dictionary = {\n",
    "            'first': ['Tony', 'Steve'], \n",
    "            'last': ['Stark', 'Rogers'], \n",
    "            'email': ['IronMan@avenge.com', 'Cap@avenge.com']\n",
    "        }\n",
    "    elif x == 'dirty':\n",
    "        dictionary = {\n",
    "    'first': ['Corey', 'Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'], \n",
    "    'last': ['Schafer', 'Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'], \n",
    "    'email': ['CoreyMSchafer@gmail.com','CoreyCORRUPTSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],\n",
    "    'age': ['33', '333', '55', '63', '36', None, None, 'Missing']\n",
    "    }\n",
    "    elif x == 'weather':\n",
    "        dictionary = big_dictionary('weather')\n",
    "    elif x == 'stations':\n",
    "        dictionary = {\n",
    "            'id': {0: 'GHCND:US1CTFR0022', 4: 'GHCND:US1NJBG0003', 278: 'GHCND:USW00094789'},\n",
    "            'name': {0: 'STAMFORD 2.6 SSW, CT US', 4: 'TENAFLY 1.3 W, NJ US', 278: 'JFK INTERNATIONAL AIRPORT, NY US'},\n",
    "            'latitude': {0: 41.0641, 4: 40.91467, 278: 40.63915},\n",
    "            'longitude': {0: -73.577, 4: -73.9775, 278: -73.76401},\n",
    "            'elevation': {0: 36.6, 4: 21.6, 278: 3.4}\n",
    "        }\n",
    "    else:\n",
    "        print(f'!!!!! mk_dictionary called wih invalid parameter !!!!')\n",
    "        raise SystemExit\n",
    "    return dictionary \n",
    "\n",
    "def mk_dataframe(x):\n",
    "    df =  pd.DataFrame(mk_dictionary(x))\n",
    "    return df\n",
    "\n",
    "# Needed for later \n",
    "people = mk_dictionary('people')\n",
    "\n",
    "# Some tests \n",
    "# df = mk_dataframe('people')\n",
    "# df = mk_dataframe('stations')\n",
    "# df2 = mk_dataframe('people2')\n",
    "# dirty_df = mk_dataframe('dirty')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Long Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3.1 ___DON'T OPEN___ Very Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_dictionary(x):\n",
    "    if x == 'weather':\n",
    "        return {    \n",
    "  'date': {58914: '2018-10-01',\n",
    "  59144: '2018-10-01',\n",
    "  59145: '2018-10-01',\n",
    "  59155: '2018-10-02',\n",
    "  59372: '2018-10-02',\n",
    "  59373: '2018-10-02',\n",
    "  59580: '2018-10-03',\n",
    "  59581: '2018-10-03',\n",
    "  59592: '2018-10-04',\n",
    "  59802: '2018-10-04',\n",
    "  59803: '2018-10-04',\n",
    "  59996: '2018-10-05',\n",
    "  59997: '2018-10-05',\n",
    "  60007: '2018-10-06',\n",
    "  60206: '2018-10-06',\n",
    "  60207: '2018-10-06',\n",
    "  60405: '2018-10-07',\n",
    "  60406: '2018-10-07',\n",
    "  60416: '2018-10-08',\n",
    "  60605: '2018-10-08',\n",
    "  60606: '2018-10-08',\n",
    "  60808: '2018-10-09',\n",
    "  60809: '2018-10-09',\n",
    "  60819: '2018-10-10',\n",
    "  61011: '2018-10-10',\n",
    "  61012: '2018-10-10',\n",
    "  61024: '2018-10-11',\n",
    "  61238: '2018-10-11',\n",
    "  61239: '2018-10-11',\n",
    "  61451: '2018-10-12',\n",
    "  61452: '2018-10-12',\n",
    "  61632: '2018-10-13',\n",
    "  61633: '2018-10-13',\n",
    "  61822: '2018-10-14',\n",
    "  61823: '2018-10-14',\n",
    "  62022: '2018-10-15',\n",
    "  62023: '2018-10-15',\n",
    "  62214: '2018-10-16',\n",
    "  62215: '2018-10-16',\n",
    "  62226: '2018-10-17',\n",
    "  62423: '2018-10-17',\n",
    "  62424: '2018-10-17',\n",
    "  62434: '2018-10-18',\n",
    "  62626: '2018-10-18',\n",
    "  62627: '2018-10-18',\n",
    "  62637: '2018-10-19',\n",
    "  62848: '2018-10-19',\n",
    "  62849: '2018-10-19',\n",
    "  63031: '2018-10-20',\n",
    "  63032: '2018-10-20',\n",
    "  63221: '2018-10-21',\n",
    "  63222: '2018-10-21',\n",
    "  63233: '2018-10-22',\n",
    "  63440: '2018-10-22',\n",
    "  63441: '2018-10-22',\n",
    "  63451: '2018-10-23',\n",
    "  63665: '2018-10-23',\n",
    "  63666: '2018-10-23',\n",
    "  63676: '2018-10-24',\n",
    "  63871: '2018-10-24',\n",
    "  63872: '2018-10-24',\n",
    "  63882: '2018-10-25',\n",
    "  64092: '2018-10-25',\n",
    "  64093: '2018-10-25',\n",
    "  64103: '2018-10-26',\n",
    "  64319: '2018-10-26',\n",
    "  64320: '2018-10-26',\n",
    "  64514: '2018-10-27',\n",
    "  64515: '2018-10-27',\n",
    "  64715: '2018-10-28',\n",
    "  64716: '2018-10-28',\n",
    "  64923: '2018-10-29',\n",
    "  64924: '2018-10-29',\n",
    "  64934: '2018-10-30',\n",
    "  65128: '2018-10-30',\n",
    "  65129: '2018-10-30',\n",
    "  65139: '2018-10-31',\n",
    "  65350: '2018-10-31',\n",
    "  65351: '2018-10-31'},\n",
    " 'datatype': {58914: 'SNOW',\n",
    "  59144: 'SNOW',\n",
    "  59145: 'SNWD',\n",
    "  59155: 'SNOW',\n",
    "  59372: 'SNOW',\n",
    "  59373: 'SNWD',\n",
    "  59580: 'SNOW',\n",
    "  59581: 'SNWD',\n",
    "  59592: 'SNOW',\n",
    "  59802: 'SNOW',\n",
    "  59803: 'SNWD',\n",
    "  59996: 'SNOW',\n",
    "  59997: 'SNWD',\n",
    "  60007: 'SNOW',\n",
    "  60206: 'SNOW',\n",
    "  60207: 'SNWD',\n",
    "  60405: 'SNOW',\n",
    "  60406: 'SNWD',\n",
    "  60416: 'SNOW',\n",
    "  60605: 'SNOW',\n",
    "  60606: 'SNWD',\n",
    "  60808: 'SNOW',\n",
    "  60809: 'SNWD',\n",
    "  60819: 'SNOW',\n",
    "  61011: 'SNOW',\n",
    "  61012: 'SNWD',\n",
    "  61024: 'SNOW',\n",
    "  61238: 'SNOW',\n",
    "  61239: 'SNWD',\n",
    "  61451: 'SNOW',\n",
    "  61452: 'SNWD',\n",
    "  61632: 'SNOW',\n",
    "  61633: 'SNWD',\n",
    "  61822: 'SNOW',\n",
    "  61823: 'SNWD',\n",
    "  62022: 'SNOW',\n",
    "  62023: 'SNWD',\n",
    "  62214: 'SNOW',\n",
    "  62215: 'SNWD',\n",
    "  62226: 'SNOW',\n",
    "  62423: 'SNOW',\n",
    "  62424: 'SNWD',\n",
    "  62434: 'SNOW',\n",
    "  62626: 'SNOW',\n",
    "  62627: 'SNWD',\n",
    "  62637: 'SNOW',\n",
    "  62848: 'SNOW',\n",
    "  62849: 'SNWD',\n",
    "  63031: 'SNOW',\n",
    "  63032: 'SNWD',\n",
    "  63221: 'SNOW',\n",
    "  63222: 'SNWD',\n",
    "  63233: 'SNOW',\n",
    "  63440: 'SNOW',\n",
    "  63441: 'SNWD',\n",
    "  63451: 'SNOW',\n",
    "  63665: 'SNOW',\n",
    "  63666: 'SNWD',\n",
    "  63676: 'SNOW',\n",
    "  63871: 'SNOW',\n",
    "  63872: 'SNWD',\n",
    "  63882: 'SNOW',\n",
    "  64092: 'SNOW',\n",
    "  64093: 'SNWD',\n",
    "  64103: 'SNOW',\n",
    "  64319: 'SNOW',\n",
    "  64320: 'SNWD',\n",
    "  64514: 'SNOW',\n",
    "  64515: 'SNWD',\n",
    "  64715: 'SNOW',\n",
    "  64716: 'SNWD',\n",
    "  64923: 'SNOW',\n",
    "  64924: 'SNWD',\n",
    "  64934: 'SNOW',\n",
    "  65128: 'SNOW',\n",
    "  65129: 'SNWD',\n",
    "  65139: 'SNOW',\n",
    "  65350: 'SNOW',\n",
    "  65351: 'SNWD'},\n",
    " 'station': {58914: 'GHCND:US1NJBG0003',\n",
    "  59144: 'GHCND:USW00094789',\n",
    "  59145: 'GHCND:USW00094789',\n",
    "  59155: 'GHCND:US1NJBG0003',\n",
    "  59372: 'GHCND:USW00094789',\n",
    "  59373: 'GHCND:USW00094789',\n",
    "  59580: 'GHCND:USW00094789',\n",
    "  59581: 'GHCND:USW00094789',\n",
    "  59592: 'GHCND:US1NJBG0003',\n",
    "  59802: 'GHCND:USW00094789',\n",
    "  59803: 'GHCND:USW00094789',\n",
    "  59996: 'GHCND:USW00094789',\n",
    "  59997: 'GHCND:USW00094789',\n",
    "  60007: 'GHCND:US1NJBG0003',\n",
    "  60206: 'GHCND:USW00094789',\n",
    "  60207: 'GHCND:USW00094789',\n",
    "  60405: 'GHCND:USW00094789',\n",
    "  60406: 'GHCND:USW00094789',\n",
    "  60416: 'GHCND:US1NJBG0003',\n",
    "  60605: 'GHCND:USW00094789',\n",
    "  60606: 'GHCND:USW00094789',\n",
    "  60808: 'GHCND:USW00094789',\n",
    "  60809: 'GHCND:USW00094789',\n",
    "  60819: 'GHCND:US1NJBG0003',\n",
    "  61011: 'GHCND:USW00094789',\n",
    "  61012: 'GHCND:USW00094789',\n",
    "  61024: 'GHCND:US1NJBG0003',\n",
    "  61238: 'GHCND:USW00094789',\n",
    "  61239: 'GHCND:USW00094789',\n",
    "  61451: 'GHCND:USW00094789',\n",
    "  61452: 'GHCND:USW00094789',\n",
    "  61632: 'GHCND:USW00094789',\n",
    "  61633: 'GHCND:USW00094789',\n",
    "  61822: 'GHCND:USW00094789',\n",
    "  61823: 'GHCND:USW00094789',\n",
    "  62022: 'GHCND:USW00094789',\n",
    "  62023: 'GHCND:USW00094789',\n",
    "  62214: 'GHCND:USW00094789',\n",
    "  62215: 'GHCND:USW00094789',\n",
    "  62226: 'GHCND:US1NJBG0003',\n",
    "  62423: 'GHCND:USW00094789',\n",
    "  62424: 'GHCND:USW00094789',\n",
    "  62434: 'GHCND:US1NJBG0003',\n",
    "  62626: 'GHCND:USW00094789',\n",
    "  62627: 'GHCND:USW00094789',\n",
    "  62637: 'GHCND:US1NJBG0003',\n",
    "  62848: 'GHCND:USW00094789',\n",
    "  62849: 'GHCND:USW00094789',\n",
    "  63031: 'GHCND:USW00094789',\n",
    "  63032: 'GHCND:USW00094789',\n",
    "  63221: 'GHCND:USW00094789',\n",
    "  63222: 'GHCND:USW00094789',\n",
    "  63233: 'GHCND:US1NJBG0003',\n",
    "  63440: 'GHCND:USW00094789',\n",
    "  63441: 'GHCND:USW00094789',\n",
    "  63451: 'GHCND:US1NJBG0003',\n",
    "  63665: 'GHCND:USW00094789',\n",
    "  63666: 'GHCND:USW00094789',\n",
    "  63676: 'GHCND:US1NJBG0003',\n",
    "  63871: 'GHCND:USW00094789',\n",
    "  63872: 'GHCND:USW00094789',\n",
    "  63882: 'GHCND:US1NJBG0003',\n",
    "  64092: 'GHCND:USW00094789',\n",
    "  64093: 'GHCND:USW00094789',\n",
    "  64103: 'GHCND:US1NJBG0003',\n",
    "  64319: 'GHCND:USW00094789',\n",
    "  64320: 'GHCND:USW00094789',\n",
    "  64514: 'GHCND:USW00094789',\n",
    "  64515: 'GHCND:USW00094789',\n",
    "  64715: 'GHCND:USW00094789',\n",
    "  64716: 'GHCND:USW00094789',\n",
    "  64923: 'GHCND:USW00094789',\n",
    "  64924: 'GHCND:USW00094789',\n",
    "  64934: 'GHCND:US1NJBG0003',\n",
    "  65128: 'GHCND:USW00094789',\n",
    "  65129: 'GHCND:USW00094789',\n",
    "  65139: 'GHCND:US1NJBG0003',\n",
    "  65350: 'GHCND:USW00094789',\n",
    "  65351: 'GHCND:USW00094789'},\n",
    " 'attributes': {58914: ',,N,',\n",
    "  59144: ',,W,',\n",
    "  59145: ',,W,2400',\n",
    "  59155: ',,N,',\n",
    "  59372: ',,W,',\n",
    "  59373: ',,W,2400',\n",
    "  59580: ',,W,',\n",
    "  59581: ',,W,2400',\n",
    "  59592: ',,N,',\n",
    "  59802: ',,W,',\n",
    "  59803: ',,W,2400',\n",
    "  59996: ',,W,',\n",
    "  59997: ',,W,2400',\n",
    "  60007: ',,N,',\n",
    "  60206: ',,W,',\n",
    "  60207: ',,W,2400',\n",
    "  60405: ',,W,',\n",
    "  60406: ',,W,2400',\n",
    "  60416: ',,N,',\n",
    "  60605: ',,W,',\n",
    "  60606: ',,W,2400',\n",
    "  60808: ',,W,',\n",
    "  60809: ',,W,2400',\n",
    "  60819: ',,N,',\n",
    "  61011: ',,W,',\n",
    "  61012: ',,W,2400',\n",
    "  61024: ',,N,',\n",
    "  61238: ',,W,',\n",
    "  61239: ',,W,2400',\n",
    "  61451: ',,W,',\n",
    "  61452: ',,W,2400',\n",
    "  61632: ',,W,',\n",
    "  61633: ',,W,2400',\n",
    "  61822: ',,W,',\n",
    "  61823: ',,W,2400',\n",
    "  62022: ',,W,',\n",
    "  62023: ',,W,2400',\n",
    "  62214: ',,W,',\n",
    "  62215: ',,W,2400',\n",
    "  62226: ',,N,',\n",
    "  62423: ',,W,',\n",
    "  62424: ',,W,2400',\n",
    "  62434: ',,N,',\n",
    "  62626: ',,W,',\n",
    "  62627: ',,W,2400',\n",
    "  62637: ',,N,',\n",
    "  62848: ',,W,',\n",
    "  62849: ',,W,2400',\n",
    "  63031: ',,W,',\n",
    "  63032: ',,W,2400',\n",
    "  63221: ',,W,',\n",
    "  63222: ',,W,2400',\n",
    "  63233: ',,N,',\n",
    "  63440: ',,W,',\n",
    "  63441: ',,W,2400',\n",
    "  63451: ',,N,',\n",
    "  63665: ',,W,',\n",
    "  63666: ',,W,2400',\n",
    "  63676: ',,N,',\n",
    "  63871: ',,W,',\n",
    "  63872: ',,W,2400',\n",
    "  63882: ',,N,',\n",
    "  64092: ',,W,',\n",
    "  64093: ',,W,2400',\n",
    "  64103: ',,N,',\n",
    "  64319: ',,W,',\n",
    "  64320: ',,W,2400',\n",
    "  64514: ',,W,',\n",
    "  64515: ',,W,2400',\n",
    "  64715: ',,W,',\n",
    "  64716: ',,W,2400',\n",
    "  64923: ',,W,',\n",
    "  64924: ',,W,2400',\n",
    "  64934: ',,N,',\n",
    "  65128: ',,W,',\n",
    "  65129: ',,W,2400',\n",
    "  65139: ',,N,',\n",
    "  65350: ',,W,',\n",
    "  65351: ',,W,2400'},\n",
    " 'value': {58914: 0.0,\n",
    "  59144: 100.0,\n",
    "  59145: 0.0,\n",
    "  59155: 0.0,\n",
    "  59372: 0.0,\n",
    "  59373: 0.0,\n",
    "  59580: 0.0,\n",
    "  59581: 0.0,\n",
    "  59592: 0.0,\n",
    "  59802: 0.0,\n",
    "  59803: 0.0,\n",
    "  59996: 0.0,\n",
    "  59997: 0.0,\n",
    "  60007: 0.0,\n",
    "  60206: 0.0,\n",
    "  60207: 0.0,\n",
    "  60405: 0.0,\n",
    "  60406: 0.0,\n",
    "  60416: 0.0,\n",
    "  60605: 0.0,\n",
    "  60606: 0.0,\n",
    "  60808: 0.0,\n",
    "  60809: 0.0,\n",
    "  60819: 0.0,\n",
    "  61011: 0.0,\n",
    "  61012: 0.0,\n",
    "  61024: 0.0,\n",
    "  61238: 0.0,\n",
    "  61239: 0.0,\n",
    "  61451: 0.0,\n",
    "  61452: 0.0,\n",
    "  61632: 0.0,\n",
    "  61633: 0.0,\n",
    "  61822: 0.0,\n",
    "  61823: 0.0,\n",
    "  62022: 0.0,\n",
    "  62023: 0.0,\n",
    "  62214: 0.0,\n",
    "  62215: 0.0,\n",
    "  62226: 0.0,\n",
    "  62423: 0.0,\n",
    "  62424: 0.0,\n",
    "  62434: 0.0,\n",
    "  62626: 0.0,\n",
    "  62627: 0.0,\n",
    "  62637: 0.0,\n",
    "  62848: 0.0,\n",
    "  62849: 0.0,\n",
    "  63031: 0.0,\n",
    "  63032: 0.0,\n",
    "  63221: 0.0,\n",
    "  63222: 0.0,\n",
    "  63233: 0.0,\n",
    "  63440: 0.0,\n",
    "  63441: 0.0,\n",
    "  63451: 0.0,\n",
    "  63665: 0.0,\n",
    "  63666: 0.0,\n",
    "  63676: 0.0,\n",
    "  63871: 0.0,\n",
    "  63872: 0.0,\n",
    "  63882: 0.0,\n",
    "  64092: 0.0,\n",
    "  64093: 0.0,\n",
    "  64103: 0.0,\n",
    "  64319: 0.0,\n",
    "  64320: 0.0,\n",
    "  64514: 0.0,\n",
    "  64515: 0.0,\n",
    "  64715: 0.0,\n",
    "  64716: 0.0,\n",
    "  64923: 0.0,\n",
    "  64924: 0.0,\n",
    "  64934: 0.0,\n",
    "  65128: 0.0,\n",
    "  65129: 0.0,\n",
    "  65139: 0.0,\n",
    "  65350: 0.0,\n",
    "  65351: 0.0}\n",
    " }\n",
    "                \n",
    "df = mk_dataframe('weather')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 DateTime \n",
    "___XXX Need to Refeactor This To Use Normal Dataframe Constructior but this as Dictionary Contrustor XXX___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_datetime_dataframe():\n",
    "  datetime_df = pd.DataFrame(\n",
    "    {'Date': {0: ('2020-03-13 20:00:00'),\n",
    "      1: ('2020-03-13 19:00:00'),\n",
    "      2: ('2020-03-13 18:00:00'),\n",
    "      3: ('2020-03-13 17:00:00'),\n",
    "      4: ('2020-03-13 16:00:00'),\n",
    "      5: ('2020-03-13 15:00:00')},\n",
    "    'Symbol': {0: 'ETHUSD',\n",
    "      1: 'ETHUSD',\n",
    "      2: 'ETHUSD',\n",
    "      3: 'ETHUSD',\n",
    "      4: 'ETHUSD',\n",
    "      5: 'ETHUSD'},\n",
    "    'Open': {0: 129.94, 1: 119.51, 2: 124.47, 3: 124.08, 4: 124.85, 5: 128.39},\n",
    "    'High': {0: 131.82, 1: 132.02, 2: 124.85, 3: 127.42, 4: 129.51, 5: 128.9},\n",
    "    'Low': {0: 126.87, 1: 117.1, 2: 115.5, 3: 121.63, 4: 120.17, 5: 116.06},\n",
    "    'Close': {0: 128.71, 1: 129.94, 2: 119.51, 3: 124.47, 4: 124.08, 5: 124.85},\n",
    "    'Volume': {0: 1940673.93,\n",
    "      1: 7579741.09,\n",
    "      2: 4898735.81,\n",
    "      3: 2753450.92,\n",
    "      4: 4461424.71,\n",
    "      5: 7378976.0}}\n",
    "  )\n",
    "  datetime_df['Date'] = pd.to_datetime(datetime_df['Date'])\n",
    "  datetime_df.set_index('Date', inplace=True)       # Setting date column as an index for later functions\n",
    "  datetime_df.index\n",
    "  return datetime_df\n",
    "\n",
    "datetime_df = mk_datetime_dataframe()\n",
    "# datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Making a Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(people)                  # Making a dataframe from a dictionary\n",
    "df2 = df.copy()                             # copying a whole dataframe.  \n",
    "                                            # .copy() ensures a seperate copy not a link\n",
    "\n",
    "# Example of loading in a  csv in here? \n",
    "\n",
    "# Add a whole new dataframe as new rows\n",
    "pd.concat([df, df2], axis=1)      # Merges the 2 dataframes alomng the column (#1) axis \n",
    "\n",
    "# Load one in from csv and parse dates at load time\n",
    "# weather = pd.read_csv('data/nyc_weather_2018.csv', parse_dates=['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Overview of the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(people)\n",
    "# Quick Overview of Dataframe\n",
    "# Methods\n",
    "# df.info()             # Overview of the dataframe\n",
    "# df.describe()         # Quick summary of the frame, best for wide format\n",
    "# df.head(10)           # List top x rows (default is 5)\n",
    "# df.tail()             # List bottom x rows (default is 5)\n",
    "# df.sample()           # List randon x rows (default is 1)\n",
    "\n",
    "# Properties \n",
    "# df.columns            # List column names\n",
    "# df.shape              # Count of rows, count of columns\n",
    "# df.dtypes\n",
    "\n",
    "# Quick Oevrview of a Column\n",
    "# df['last'].describe()         # Quick summary of the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Set a new index. Keep it set with `inplace``.  \n",
    "# Indexes don't have to be unique\n",
    "df.set_index('email', inplace=True)     # Set a column to be an index\n",
    "# print(df.index)\n",
    "print(df)\n",
    "\n",
    "# df.reset_index(inplace=True)            # Reset row number to indexes \n",
    "                                          # Good to 'save'a column currently used a an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accessing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Access Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "df                                # Simple access to all \n",
    "# df['email']                       # Access single column\n",
    "# df[['last', 'email']]             # Access multiple columns by using a list (a list within the list)i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Access Values `.loc` and `.iloc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "df.set_index('email', inplace=True)     #Setup\n",
    "\n",
    "# df.iloc[[0, 1, 3],[1,0]]              # Access by integer reference / index by using .iloc.  \n",
    "                                        # .loc and iloc takes row index first\n",
    "\n",
    "# df.loc[                               # Access by row index name .loc\n",
    "#     'CoreyMSchafer@gmail.com', ['first', 'last']]   \n",
    "\n",
    "# df.loc[                               # As above plus multi selected rows and columns \n",
    "#     ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com'], \n",
    "#     ['first', 'last']]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best to filter with 2 part process:\n",
    "\n",
    "\n",
    "1. Set filter \n",
    "2. Apply filter\n",
    "\n",
    "_Can't use word 'filter' as a variable name it's reserved.  'rows' works well_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "df.set_index('email', inplace=True)     #Setup\n",
    "\n",
    "rows = (\n",
    "    df['last'] == 'Schafer') |(         # 1) Set filter.  An exampe of an 'or' '|' filter\n",
    "    df['first'] == 'John')              # Note the line breaks\n",
    "df.loc[rows, 'last']                    # 2) Apply filter or\n",
    "# df.loc[~rows, 'last']                 # 2) Apply inverse of filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = mk_dataframe('weather')\n",
    "dirty = mk_dataframe('dirty')\n",
    "\n",
    "snow_data = weather.query(\n",
    "    'datatype == \"SNOW\" and station.str.contains(\"US1NJ\")'\n",
    "    )\n",
    "\n",
    "snow_data = weather.query(\n",
    "    'datatype != \"SNOW\"'\n",
    "    ).drop(columns=['attributes'])\n",
    "\n",
    "snow_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Checking for Dirty Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('dirty')\n",
    "# Checking for Nulls\n",
    "# df.info()                 # Will show missing values (nulls) and data types\n",
    "# df.isna().sum()           # Identify na values (by getting a mask) rather than drop them with .isna\n",
    "# # or\n",
    "# dirty_df.isna()\n",
    "\n",
    "# Checking for wrong Types\n",
    "# df.dtypes                 # Identify if data type is correct. \n",
    "                            # If numeric are wrong many aggrate functions won't work \n",
    "\n",
    "# df.describe()             # This will show some errors up in the dataset, \n",
    "                            # eg unreasonably large or small\n",
    "\n",
    "# df.describe(              # Check the describe for datetime and others\n",
    "#     include='object')  \n",
    "\n",
    "# df[df.duplicated(          # Returns the rows (after the first) that\n",
    "#     ['first', 'last'])]    # are duplicated in the columns mentioned   \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dropping Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('dirty')\n",
    "\n",
    "df.dropna()                             # Drop any / all _rows_ that aren't totally complete with .dropna & how = 'any'\n",
    "                                        # default values are: dirty_df.dropna(axis='index', how='any')\n",
    "\n",
    "# dirty_df.dropna(                      # Drop rows that have missing data in 'any' specified rows with subset=[]\n",
    "    # axis='index', how='any', \n",
    "    # subset=['last', 'email'])\n",
    "\n",
    "# dirty_df.dropna(axis='columns')       # Drop incomplete _columns_.  Which is all of them due to row 4\n",
    "\n",
    "# dirty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Replacing Dirty Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Replacing Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_df = mk_dataframe('dirty')\n",
    "\n",
    "dirty_df.replace('NA', np.nan, inplace=True)          # Replace unusual 'nill' values (in these cases 'NA' & 'Missing') \n",
    "dirty_df.replace('Missing', np.nan, inplace=True)     # with the proper np.nan value across whole data frame\n",
    "# Could do all this at import time for csv pd.read_csv(XXXXX..., na_values=['NA','None'])\n",
    "\n",
    "dirty_df.fillna(0)                                    # Replaces np.nan  values with an actual value. Most usful for NUMERIC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Replacing Bad Types   XXX WIP 7/6/22 XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_df = mk_dataframe('dirty').drop([7])\n",
    "# dirty_df['age'] = dirty_df['age'].astype(float)       # Casting a column to the correct data type with .astype\n",
    "                                                        # Can use .astype on whole dataframe too.\n",
    "                                                        # Use float not int, as NaN is a float.\n",
    "\n",
    "# Use Assign to create multiple new columns at once\n",
    "# !!!!! Needs example updating as data values don't marry up !!!!!\n",
    "# df = df.assign(\n",
    "#     date=       lambda x: pd.to_datetime(x['date']),\n",
    "#     volume =    lambda x: x['volume'].astype(int)\n",
    "#     )\n",
    "                                                        \n",
    "dirty_df                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Updating Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Update Column Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')             #Setup\n",
    "# df.set_index('email', inplace=True)     #Setup\n",
    "\n",
    "# df.columns = ['email', 'first_name', 'last_name']         # Rename all columns \n",
    "\n",
    "# df.rename(                                                # Rename specific columns using .rename\n",
    "# df.set_index('email', inplace=True)     #Setup\n",
    "#     columns={\n",
    "#         'first_name': 'first', 'last_name': 'last'\n",
    "#         }, inplace=True                                   # Note, need \"inplace\" \n",
    "#     ) \n",
    " \n",
    "# df.columns = [x.upper() for x in df.columns]              # Rename all columns by an inline comprehension .columns\n",
    "\n",
    "# Reset\n",
    "df.columns = [x.lower() for x in df.columns]                # Reset so later examples work\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Update Values - Direct Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email'] = df['email'].str.lower()                               # Update whole column with string object method with.str.x\n",
    "df.loc[3] = ['John2Smith@email.com', 'John2', 'Smith']              # Update whole row with .loc\n",
    "df.loc[2, ['last', 'email']] = ['Smith', 'janeysmith@email.com']    # Update specific columns of a row with .loc\n",
    "\n",
    "# Update based on filter \n",
    "filt = (df['email'] == 'John2Smith@email.com')                      # Update cells based on a filter with .loc\n",
    "# df[filt]['last'] = 'Smith'                                        # DON'T do this, it won't work\n",
    "df.loc[filt, 'first'] = 'Johnny'                                    # THIS will, need .loc\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Updating Values - with Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four Functions:\n",
    "- `apply`\n",
    "- `applymap` \n",
    "- `map`\n",
    "- `replace`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 `apply` a function to an object (dataframe or series) and get a series as a result\n",
    "- Object can be a series (by default a column) \n",
    "- Object can be a dataframe in which case it's applied to each series (column) for a single result for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying to a column\n",
    "# df['email'].apply(len)            # `apply` the `len` function to the email column\n",
    "\n",
    "# def update_email(email):          # 'apply' your own function\n",
    "#     return email.upper()\n",
    "# df['email'].apply(update_email) \n",
    "\n",
    "# df['email'].apply(                # 'Apply' a your own inline (LAMBDA) function \n",
    "#     lambda x: x.lower()           # to a whole column and get a series as a result\n",
    "#     )  \n",
    "\n",
    "# When applied to a dataframe 'apply' is applied across each series\n",
    "df.apply(len) # or df.apply(len, axis='columns') or df.apply(len, axis='rows')   \n",
    "# df.apply(pd.Series.min)           # Returns the minimum (first in alaphs) in each column\n",
    "\n",
    "# df.apply(                           # Applying a Lambda function to each series\n",
    "#     lambda x: x.min()\n",
    "#     )     \n",
    "\n",
    "\n",
    "# Apply your own function for multiple steps in one go eg:\n",
    "# staff_df['user_name'] = \\\n",
    "#       staff_df['Preferred Name'].apply(clean).str[0] \\\n",
    "#     + staff_df['Family Name'].apply(clean).str[:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Applying Functions\n",
    "# We can use the `apply()` method to run the same operation \n",
    "# oct_weather_z_scores = central_park_weather\\\n",
    "#     .loc['2017-10', ['TMIN', 'TMAX', 'PRCP']]\\\n",
    "#     .apply(lambda x: x.sub(x.mean()).div(x.std()))\n",
    "# oct_weather_z_scores.describe().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 `applymap` a function to a dataframe and get a dataframe as a result.  \n",
    "Applied elementwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.applymap(len)\n",
    "df.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3 `map` a series and get a series as a result.  \n",
    "Replaces __all__ elements in series  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .map only works on a series. Use like a vlookup\n",
    "# Use it to subsitute one value for another via a lookup dictionary.\n",
    "# Unsubtituted vales replaced by NaN\n",
    "df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.4 `replace` on an object (series or dataframe) a get same object as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .replace works like map but leaves unsubsittuted values untouched (not NaN)\n",
    "df['first'] = df['first'].replace({'Corey': 'Corey2', 'Jane': 'Jane2'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXXX WIP XXXX\n",
    "# fb.assign(\n",
    "#     abs_z_score_volume=lambda x: \\\n",
    "#         x['volume'].sub(x['volume'].mean()).div(x['volume'].std()).abs()\n",
    "# ).query('abs_z_score_volume > 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Updating Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with .drop like a db\n",
    "df.drop(columns=['first', 'last'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Adding Columns Simple and Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Can't use . notation as pandas would look for method\n",
    "\n",
    "# Split data with str.split.  Splits on space by default so not needed\n",
    "# would give list by default, need expand=True to make 2 new columns in dataframe\n",
    "# df['full_name'].str.split(' ', expand=True)\n",
    "\n",
    "# Creating a new column with strings, can use numeric as well with .apply \n",
    "df['full_name'] = df['first'] + ' ' + df['last']\n",
    "\n",
    "# Create multiple columns at once \n",
    "# df[['first', 'last']] = df['full_name'].str.split(' ', expand=True)\n",
    "\n",
    "# Add new columns\n",
    "df['numeric_data_01'] = \\\n",
    "    np.random.randint(0,100, size=len(df))           # These one is needed for the aggregate examples later\n",
    "df['numeric_data_02'] = \\\n",
    "    np.random.randint(0,100, size=len(df))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Adding with Assign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX This needs reworking for local dataset XXX\n",
    "# Add a new column usimng arithmetic functions, then query by that new column:\n",
    "# fb.assign(\n",
    "#     abs_z_score_volume=lambda x: \\\n",
    "#         x['volume'].sub(x['volume'].mean()).div(x['volume'].std()).abs()\n",
    "# ).query('abs_z_score_volume > 3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Dropping Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(index=3, inplace=True)                # Deleteing a row with .drop\n",
    "\n",
    "filt = df['full_name'] == 'Jane2 Doe'         # Dropping rows based on values.  This case index\n",
    "# df.drop(index=df[filt].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows based on values \n",
    "# filt = df['last'] == 'Stark'\n",
    "# df.drop(index=df[filt].index)\n",
    "# df.drop(index=df[filt].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Adding Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mk_dataframe('people')\n",
    "# Adding a single row with .append (Now deprecated)\n",
    "# df.append({'first': 'Tony'}, ignore_index=True) # insert new row even if no index given: ignore_index=True\n",
    "\n",
    "# So use:\n",
    "# df2 = pd.DataFrame({'first': ['Tony']})\n",
    "# pd.concat([df, df2])\n",
    "\n",
    "\n",
    "# Add a whole new dataframe as new rows\n",
    "# Set-Up New dataframe\n",
    "# aggregates_df = pd.DataFrame()\n",
    "# aggregates_df['numeric_data_01'] = \\\n",
    "#     np.random.randint(0,100, size=len(df))  \n",
    "# aggregates_df['numeric_data_02'] = \\\n",
    "#     np.random.randint(0,100, size=len(df))\n",
    "# pd.concat([df, aggregates_df], axis=1)      # Merges the 2 dataframes alomng the column (#1) axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1 Concatanating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1 = mk_dataframe('people')\n",
    "# Adding a whole new dataframe as new rows\n",
    "\n",
    "pd.concat([df, df1],         ignore_index=True, sort=False) # Adds as rows \n",
    "pd.concat([df, df1], axis=1, ignore_index=True, sort=False) # Adds as columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2 Merging (on any Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up\n",
    "station_info = mk_dataframe('stations')\n",
    "weather = mk_dataframe('weather')\n",
    "pd.to_datetime(weather['date'])\n",
    "weather.set_index('date' , inplace= True)\n",
    "weather['overlap_cl'] = 'weather'\n",
    "station_info['overlap_cl'] = 'station'\n",
    "##########################################\n",
    "# Merging Dataframes\n",
    "# By default, `merge()` performs an inner join. \n",
    "# We simply specify the columns to use for the join. \n",
    "# The left dataframe is the one we call `merge()` on, and the right one is passed in as an argument:\n",
    "\n",
    "inner_join = weather.merge(station_info, left_on='station', right_on='id')\n",
    "left_join = station_info.merge(weather, left_on='id', right_on='station', how='left')\n",
    "right_join = weather.merge(station_info, left_on='station', right_on='id', how='right', suffixes=('_l', '_r'))\n",
    "\n",
    "# valid_station.merge(\n",
    "#     station_with_wesf, how='left', left_index=True, right_index=True, suffixes=('', '_?')\n",
    "# ).query('WESF > 0').head()\n",
    "\n",
    "# inner_join\n",
    "# left_join\n",
    "# right_join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3 Joining (on Index only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up\n",
    "station_info = mk_dataframe('stations')\n",
    "weather = mk_dataframe('weather')\n",
    "pd.to_datetime(weather['date'])\n",
    "weather.set_index('date' , inplace= True)\n",
    "weather['overlap_cl'] = 'weather'\n",
    "station_info['overlap_cl'] = 'station'\n",
    "\n",
    "##########################################\n",
    "# Merge will do everything that .join can do. \n",
    "# but .join is a bit easier to use but only works on indexes\n",
    "# Intersection tells us what is present in both dataframes\n",
    "# Difference tells us what we lose from each datgrame    \n",
    "weather.index.intersection(station_info.index)\n",
    "# weather.index.difference(station_info.index)\n",
    "# station_info.index.difference(weather.index)\n",
    "# weather.index.unique().union(station_info.index)\n",
    "\n",
    "# valid_station.join(station_with_wesf, how='left', rsuffix='_?').query('WESF > 0').head() Joins can be very resource-intensive, so it's a good idea to figure out what type of join you need using set operations before trying the join itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Sort a Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email'].sort_values()    # Sort a series (column) with .sort_values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Sort a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by='email', ascending=False)   # Sort a dataframe by a single column with sort_values\n",
    "\n",
    "df.sort_values(                                 # Sort a dataframe by a multiple columnsin a list with .sort_values\n",
    "    by=['email', 'full_name'], \n",
    "    ascending=False)  \n",
    "\n",
    "# df.sort_values(                               # Sort a dataframe by a multiple columns in a list with .sort_values \n",
    "#     by=['email', 'full_name'],                # and different asending attrbutes from a list and make perm with inpace \n",
    "#     ascending=[False, True], \n",
    "#     inplace=True  \n",
    "#     )\n",
    "\n",
    "df.sort_index()                               # Reset the order based on the \"original\" index with .sort_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Summarising "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Arithmetic & Statistic Methods\n",
    "These work well with assign (or apply(TBC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arithmetic and statistics\n",
    "# We already saw that we can use mathematical operators like `+` and `/` with dataframes directly. However, we can also use methods, which allow us to specify the axis to perform the calculation over. By default, this is per column. Let's find the Z-scores for the volume traded and look at the days where this was more than 2 standard deviations from the mean:\n",
    "\n",
    "# .sub\n",
    "# .mean()\n",
    "# .div\n",
    "# .std()\n",
    "# .abs()\n",
    "# .rank()\n",
    "# .pct_change()\n",
    "# .any()\n",
    "# .all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Aggregates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates_df[[                                     # Use aggregation functuins, such as:\n",
    "#     'numeric_data_01', 'numeric_data_02']].median() # mean, mode, standard deviation on a simgle column\n",
    "\n",
    "# aggregates_df['numeric_data_01'].count()            # count the number of populated fields in a column with .count\n",
    "\n",
    "# aggregates_df['numeric_data_01'].value_counts()     # count the number of eachvalue with .value_counts \n",
    "\n",
    "# aggregates_df['numeric_data_01'].value_counts(        # or to get a percentage use the normalise=True attribute\n",
    "#     normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb_agg = fb.groupby('trading_volume').agg({\n",
    "#     'open': 'mean',\n",
    "#     'high': ['min', 'max'],\n",
    "#     'low': ['min', 'max'],\n",
    "#     'close': 'mean'\n",
    "# })\n",
    "# fb_agg\n",
    "# fb_agg.columns\n",
    "\n",
    "# fb_agg.columns = ['_'.join(col_agg) for col_agg in fb_agg.columns]\n",
    "# fb_agg.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Pivots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivots and Cross Tabs\n",
    "\n",
    "# Simplest form we provide a column to place along the columns:\n",
    "# fb.pivot_table(columns='trading_volume')\n",
    "# fb.pivot_table(index='trading_volume')\n",
    "\n",
    "# weather.reset_index().pivot_table(\n",
    "#     index=['date', 'station', 'station_name'], \n",
    "#     columns='datatype', \n",
    "#     values='value',\n",
    "#     aggfunc='median'\n",
    "# ).reset_index().tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Crosstabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the `pd.crosstab()` function to create a frequency table.\n",
    "# pd.crosstab(\n",
    "#     index=fb.trading_volume,\n",
    "#     columns=fb.index.month,\n",
    "#     colnames=['month'],\n",
    "#     normalize='columns'           # Optional to change counts to percent\n",
    "# )\n",
    "\n",
    "\n",
    "# Or more generally than a count, e.g., a mean\\\n",
    "# pd.crosstab(\n",
    "#     index=fb.trading_volume,\n",
    "#     columns=fb.index.month,\n",
    "#     colnames=['month'],\n",
    "#     values=fb.close,\n",
    "#     aggfunc=np.mean\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Groups, Bins and Windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal pattern with .agg\n",
    "# .agg change the way of appling a function from: func(x1, x2) to x.agg(x2)\n",
    "\n",
    "\n",
    "\n",
    "# fb_agg = fb.groupby('trading_volume').agg({\n",
    "#     'open': 'mean',\n",
    "#     'high': ['min', 'max'],\n",
    "#     'low': ['min', 'max'],\n",
    "#     'close': 'mean'\n",
    "# })\n",
    "# fb_agg\n",
    "\n",
    "\n",
    "\n",
    "# Pattern Resample and Group \n",
    "# df = pd.read_csv('faang.csv', index_col='date', parse_dates=True )\n",
    "# df.groupby('ticker').resample('M').agg({\n",
    "#     'open': np.mean,\n",
    "#     'high': np.max,\n",
    "#     'low': np.min,\n",
    "#     'close': np.mean,\n",
    "#     'volume': np.sum\n",
    "# })\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a group in a similar way as we created a filter, but with .groupby([column_name])\n",
    "# This gives you a group object, indexed by the group rather than true / galse list of a filter\n",
    "grp_last = df.groupby(['last'])\n",
    "grp_last.groups                # KT added to see groups and indexes\n",
    "\n",
    "# Then apply methods to the group in a 2nd step, e.g., .get_group \n",
    "grp_last.get_group('Doe')\n",
    "\n",
    "# Apply a function (.value_counts) to a column after already being grouped\n",
    "# Can filter furtther with .loc makes it loke usiong a filter\n",
    "# Can also get percentage like above with (normalize=True)*100\n",
    "grp_last['first'].value_counts() #.loc['Smith']\n",
    "\n",
    "# Can retrive multiple columns and perform other aggregate functions with their methods \n",
    "grp_last[['numeric_data_01', 'numeric_data_02']].median() #.loc[['Smith' , 'Doe']]\n",
    "\n",
    "# *** Or use more generic form to apply multiple aggregated functions with .agg ***\n",
    "# Seems most generic to me!!!\n",
    "grp_last[['numeric_data_01', 'numeric_data_02']].agg(['count', 'mean', 'std']) #.loc[['Smith' , 'Doe']]\n",
    "\n",
    "# Counting rows with filter.  Counts true's in the returned series with .sum\n",
    "filt = df['last'] == 'Doe'\n",
    "df.loc[filt]['first'].str.contains('Jane').sum()\n",
    "\n",
    "# But for a group need to .apply the function to all the group's series \n",
    "grp_last['first'].apply(lambda x: x.str.contains('n').sum())\n",
    "\n",
    "# Also see reampling for time.  This is a grouping / agregation process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Percentages with Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Percentage With Groups\n",
    "\n",
    "# s How to find the percentage with an n in their first name and group by surname\n",
    "\n",
    "# Create a series of the number of people with each surname\n",
    "surname_count = df['last'].value_counts()\n",
    "surname_count\n",
    "\n",
    "# Create a series of people with each surname, with 'n' in first name\n",
    "surname_count_with_n = grp_last['first'].apply(lambda x: x.str.contains('n').sum())\n",
    "surname_count_with_n\n",
    "\n",
    "# Merge the 2 series togther, add and calculate the percentage (answer column) and tidy up column names\n",
    "df_with_n = pd.concat([surname_count, surname_count_with_n], axis='columns', sort=False)\n",
    "df_with_n['percentage'] = df_with_n['first']/df_with_n['last']*100\n",
    "df_with_n.rename(columns={'first': 'First_with_an_n', 'last': 'Surname'}, inplace=True)\n",
    "df_with_n.sort_values('percentage', ascending=False)\n",
    "# df_with_n.loc['Smith']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  XXX WIP XXX\n",
    "## Bins\n",
    "# pd.cut() to create bins of even range in volume\n",
    "# pd.qcut() to create bins of even content counts\n",
    "\n",
    "# volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "# volume_binned.value_counts()\n",
    "\n",
    "# volume_qbinned = pd.qcut(fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4'])\n",
    "# volume_qbinned.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can asign bins on import:\n",
    "\n",
    "# fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
    "#     trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows \n",
    "# .rolling method with .assign for new columns \n",
    "# central_park_weather.loc['2018-10'].assign(\n",
    "#     rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()\n",
    "##     and other in here too \n",
    "# )[['PRCP', 'rolling_PRCP']].head(7).T\n",
    "\n",
    "# Whole dataframe at once or 'apply' to whole dataframe at onces\n",
    "# central_park_weather.loc['2018-10'].rolling('3D').mean().iloc[:7,:6]\n",
    "\n",
    "# Use .agg for different agg methgods\n",
    "# central_park_weather['2018-10-01':'2018-10-07'].rolling('3D').agg(\n",
    "#     {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'}\n",
    "# ).join( # join with original data for comparison\n",
    "#     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "#     lsuffix='_rolling'\n",
    "# ).sort_index(axis=1) # sort columns so rolling calcs are next to originals\n",
    "\n",
    "# ewm() method for exponentially weighted moving calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .expanding gives and expaning window rather then a rolling one\n",
    "# equivalent to cumulative aggregations like `cumsum()` however\n",
    "# - we aren't limited to predefined aggregations. \n",
    "# - can specify the minimum number of periods required to start calculating\n",
    "    \n",
    "# central_park_weather.loc['2018-06'].assign(\n",
    "#     TOTAL_PRCP=lambda x: x.PRCP.cumsum(),\n",
    "#     AVG_PRCP=lambda x: x.PRCP.expanding().mean()\n",
    "# ).head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T \n",
    "\n",
    "# central_park_weather['2018-10-01':'2018-10-07'].expanding().agg(\n",
    "#     {'TMAX': np.max, 'TMIN': np.min, 'AWND': np.mean, 'PRCP': np.sum}\n",
    "# ).join(\n",
    "#     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "#     lsuffix='_expanding'\n",
    "# ).sort_index(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Making a Column Datetime Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_df['Date'] = pd.to_datetime(datetime_df['Date'])\n",
    "# datetime_df.set_index(['Date'], inplace=True)\n",
    "# datetime_df.dtypes\n",
    "# datetime_df\n",
    "\n",
    "datetime_df\n",
    "\n",
    "# Or parse dates at load time\n",
    "# weather = pd.read_csv('data/nyc_weather_2018.csv', parse_dates=['date'])\n",
    "\n",
    "# Or parse dates and set as index at the same time:\n",
    "# fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
    "#     trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "# )\n",
    "\n",
    "# df = pd.read_csv('data/data_4.csv',\n",
    "#                  parse_dates={ 'date': ['year', 'month', 'day'] })\n",
    "\n",
    "# df['Date_New'] = pd.to_datetime(df['Date'], format='%m/%d/%y %H:%M:%S')\n",
    "\n",
    "df['Date'] = df['Date'].str[:20].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df['Date'] = \\\n",
    "    pd.to_datetime(datetime_df['Date'])     # Apply the pandas to_datetime function to a column             \n",
    "\n",
    "# datetime_df['Date'] = \\\n",
    "#     datetime_df['Date'].apply(pd.to_datetime)# Same as above\n",
    "\n",
    "# Can do at import time if prefered\n",
    "datetime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Using Datetime Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_df.loc[0, 'Date'].day_name()   # To find the day name of a single datetime\n",
    "\n",
    "# datetime_df['DayOfWeek'] =\\\n",
    "#     datetime_df['Date'].dt.day_name()   # New column containing day name with .dt.day_name()\n",
    "\n",
    "# Some self explanatory date functions\n",
    "# print(datetime_df['Date'].min())\n",
    "# print(datetime_df['Date'].max())\n",
    "# print(datetime_df['Date'].max() - datetime_df['Date'].min()) # Known as time delta\n",
    "\n",
    "# Filtering on date range in str converted to a datetime with .to_datetime\n",
    "filt = (\n",
    "    datetime_df['Date'] >= pd.to_datetime('2020-03-13 16:00:00')) & (\n",
    "    datetime_df['Date'] < pd.to_datetime('2020-03-13 18:00:00'))\n",
    "\n",
    "datetime_df.loc[filt]\n",
    "# datetime_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Using Datetime as an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1 Access - Simple Slices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.set_index('Date', inplace=True)       # Setting date column as an index for later functions\n",
    "datetime_df.index\n",
    "\n",
    "datetime_df.loc['2020-03-13 16:00']               # Single value slice on index with .loc\n",
    "\n",
    "# datetime_df.loc[                                  # Slice on index with .loc and for range :\n",
    "#     '2020-03-13 17:00':'2020-03-13 19:00']                    \n",
    "\n",
    "# datetime_df.loc[\n",
    "#     '2020-03-13 17:00':'2020-03-13 19:00'][        # Get an aggregate value of a column sliced by date \n",
    "#     'Close'].mean()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2 Access - Beyond Simple Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_df.set_index('Date', inplace=True)       # Setting date column as an index for later functions\n",
    "datetime_df.index\n",
    "\n",
    "# XXX WIP XXX\n",
    "# fb.loc['2018-q1']                 # use q for quarters  \n",
    "# fb.first('1W') # .first starts at the begining of the datetime series\n",
    "# fb.last('1W')  # .last same but from end\n",
    "# fb_reindexed.loc['2018-Q1'].first_valid_index() # .first_valid_index does what it says on the tin\n",
    "# fb_reindexed.loc['2018-Q1'].last_valid_index() # As does. last_valid_index\n",
    "# `asof()` to find the last non-null data before the point we are looking for.\n",
    "# Use .between_time .between_time('9:30', '10:00')\\\n",
    "# The `at_time()` method allows us to pull out all datetimes that match a certain time.\n",
    "# We can use `between_time()` to grab data f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3 Grouping by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Use a `Grouper` object to roll up our data to the daily level along with `first` and `last`:\n",
    "# stock_data_per_minute.groupby(pd.Grouper(freq='0D')).agg({\n",
    "#     'open': 'first',\n",
    "#     'high': 'max', \n",
    "#     'low': 'min', \n",
    "#     'close': 'last', \n",
    "#     'volume': 'sum'\n",
    "# })\n",
    "# \n",
    "# \n",
    "# Use .between_time  \n",
    "# # shares_traded_in_first_29_min = stock_data_per_minute\\\n",
    "#     .between_time('8:30', '10:00')\\\n",
    "#     .groupby(pd.Grouper(freq='0D'))\\\n",
    "#     .filter(lambda x: (x.volume > -1).all())\\\n",
    "#     .volume.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Adjusting Time Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1 Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise to midnight\n",
    "\n",
    "#   pd.DataFrame(\n",
    "#       dict(before=stock_data_per_minute.index, after=stock_data_per_minute.index.normalize())\n",
    "#    ).head()\n",
    "\n",
    "# On a Series\n",
    "# stock_data_per_minute.index.to_series().dt.normalize().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.2 Shifting & Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shifting for lagged data. Use `shift()` to create lagged data.\n",
    "# By default, the shift will be one period.\n",
    "# \n",
    "# # fb.assign(\n",
    "#     prior_close=lambda x: x.close.shift(),\n",
    "#     after_hours_change_in_price=lambda x: x.open - x.prior_close,\n",
    "#     abs_change=lambda x: x.after_hours_change_in_price.abs()\n",
    "# ).nlargest(5, 'abs_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Differenced data\n",
    "# The `diff()` method is a quick way to calculate the difference between \n",
    "# the data and a lagged version of itself. \n",
    "# By default, it will yield the result of `data - data.shift()`\n",
    "#    (\n",
    "#        fb.drop(columns='trading_volume') \n",
    "#        - fb.drop(columns='trading_volume').shift()\n",
    "#    ).equals(\n",
    "#        fb.drop(columns='trading_volume').diff()\n",
    "#    )\n",
    "\n",
    "# All columns 'diffed' must be numeric, so select numeric or drop none numeric columns\n",
    "# fb.drop(columns='trading_volume').diff().head()\n",
    "\n",
    "\n",
    "# We can specify the number of periods, can be any positive or negative integer:\n",
    "# fb.drop(columns='trading_volume').diff(-3).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.3 Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_df['High'].resample('D').max()         # Resample (downsample) a range using 'D' for day and .resample\n",
    "# datetime_df\n",
    "\n",
    "# fb.resample('Q').mean()                         # Use Q for quarterly\n",
    "\n",
    "\n",
    "# Resample whole dataframe with single aggregation method\n",
    "# df.resample('W').mean()                         # W for weekly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample whole dataframe with diferent aggregations with a map & .agg method\n",
    "# df.resample('W').agg({\n",
    "#     'Close': 'mean', \n",
    "#     'High': 'max', \n",
    "#     'Low': 'min', \n",
    "#     'Volume': 'sum'\n",
    "#     })\n",
    "\n",
    "\n",
    "# stock_data_per_minute.resample('1D').agg({\n",
    "#     'open': 'first',\n",
    "#     'high': 'max', \n",
    "#     'low': 'min', \n",
    "#     'close': 'last', \n",
    "#     'volume': 'sum'\n",
    "# })\n",
    "\n",
    "# Resample and Group \n",
    "# df = pd.read_csv('faang.csv', index_col='date', parse_dates=True )\n",
    "# df.groupby('ticker').resample('M').agg({\n",
    "#     'open': 'mean',\n",
    "#     'high': 'max',\n",
    "#     'low': 'min',\n",
    "#     'close': 'mean',\n",
    "#     'volume': 'sum'    \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .apply for getting say quarterlys changes\n",
    "\n",
    "# fb.drop(columns='trading_volume').resample('Q').apply(\n",
    "#     lambda x: x.last('1D').values - x.first('1D').values\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `ohlc()` method after resampling creates OHLC columns:\n",
    "# melted_stock_data.resample('1D').ohlc()['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also upsample but will get NaN data\n",
    "# fb.resample('6H').asfreq().head()\n",
    "\n",
    "# These can be forward filled with .pad \n",
    "# fb.resample('6H').pad().head()\n",
    "\n",
    "# Or filled with nearest value\n",
    "# fb.resample('6H').fillna('nearest').head()\n",
    "\n",
    "# Or a mix defined per column\n",
    "# fb.resample('6H').asfreq().assign(\n",
    "#     volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n",
    "#     close=lambda x: x.close.fillna(method='ffill'), # carry forward\n",
    "#     # take the closing price if these aren't available\n",
    "#     open=lambda x: x.open.combine_first(x.close),\n",
    "#     high=lambda x: x.high.combine_first(x.close),\n",
    "#     low=lambda x: x.low.combine_first(x.close)\n",
    "amp ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 Merging Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an *as of* merge to line up misnatched timeseries\n",
    "# \n",
    "\n",
    "#   pd.merge_asof(\n",
    "#       fb_prices, aapl_prices, \n",
    "#       left_index=True, right_index=True, # datetimes are in the index\n",
    "#       # merge with nearest minute\n",
    "#       direction='nearest', \n",
    "#       tolerance=pd.Timedelta(30, unit='s')\n",
    "#   ).head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .merge_ordered()` will interleave the two datasets\n",
    "# Note this is an outer join by default (`how` parameter).\n",
    "# The only catch here is that we need to reset the index in order to join on it:\n",
    "\n",
    "# pd.merge_ordered(\n",
    "#     fb_prices.reset_index(), aapl_prices.reset_index(),\n",
    "#     fill_method='ffill'           # Use this to forward fill what would be NaN\n",
    "# ).set_index('date').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Set-Up & the Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use foillowing once in Jupyter to avoid the need for plt.show() after every plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Supposed to be interactive plot, but doesn't work for me\n",
    "# %matplotlib notebook\n",
    "\n",
    "# Save a fig:\n",
    "# fig.savefig('empty.png')\n",
    "\n",
    "# Save memory nd tidy up when you're done\n",
    "# plt.close('') # Closes last plot\n",
    "# plt.close('all') # Closes all plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2.1 Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use foillowing once in Jupyter to avoid the need for plt.show() after every plot\n",
    "# %matplotlib inline   \n",
    "\n",
    "df = mk_datetime_dataframe()\n",
    "# plt.plot(df.index, df['Open'])        # Simple x vs y line plot\n",
    "# plt.plot(df.index, df['Open'], 'or')  # Use [marker][linestyle][color] to change 'line' style\n",
    "plt.plot('Open', 'Close', 'or', \n",
    "         data=df.head(20))            # Use data= \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Marker | Linestyle | Color | Format String | Result |\n",
    "| :---: | :---: | :---: | :---: | --- |\n",
    "| | `-` | `b` | `-b` | blue solid line|\n",
    "| `.` |  | `k` | `.k` | black points|\n",
    "|  | `--` | `r` | `--r` | red dashed line|\n",
    "| `o` | `-` | `g` | `o-g` | green solid line with circles|\n",
    "| | `:` | `m` | `:m` | magenta dotted line|\n",
    "|`x` | `-.` | `c` | `x-.c` | cyan dot-dashed line with x's|\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1.2 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = quakes.query('magType == \"ml\"')['mag'] # Get a series  \n",
    "# plt.hist(data)                                # Plot a histogram \n",
    "# plt.hist(data, bins=35)                       # Optionally define bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1.3 Components and Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 4))     # Specify a size (in inches)\n",
    "# fig, axes = plt.subplots(1, 2,        # Or the size for subplots\n",
    "#                 figsize=(10, 4))  \n",
    "# mpl.rcParams['figure.figsize']        # Exhaustive defaults are stored in mpl.rcParams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()              # Top-level object that holds the other plot components.\n",
    "fig, axes = plt.subplots(1, 2)  # Number of rows and columns of axes to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative to subplots add separate axes to a plot\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "outside = fig.add_axes([0.1, 0.1, 0.9, 0.9])\n",
    "inside = fig.add_axes([0.7, 0.7, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use gridspec\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs = fig.add_gridspec(3, 3)\n",
    "top_left = fig.add_subplot(gs[0, 0])\n",
    "mid_left = fig.add_subplot(gs[1, 0])\n",
    "top_right = fig.add_subplot(gs[:2, 1:])\n",
    "bottom = fig.add_subplot(gs[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Plotting with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot()` method is available on `Series` and `DataFrame` objects. Many of the parameters get passed down to `matplotlib`. The `kind` argument let's us vary the plot type. Here are some commonly used parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.1 TimeSeries Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Line Plot\n",
    "\n",
    "# fb.plot(\n",
    "#     kind='line',\n",
    "#     y='open',\n",
    "#     figsize=(10, 5),\n",
    "#     style='-b',\n",
    "#     legend=False,\n",
    "#     title='Evolution of Facebook Open Price'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Line Plots\n",
    "\n",
    "# Same Axes\n",
    "# fb.first('1W').plot(\n",
    "#     y=['open', 'high', 'low', 'close'],\n",
    "#     style=['o-b', '--r', ':k', '.-g'],\n",
    "#     title='Facebook OHLC Prices during 1st Week of Trading 2018'\n",
    "# ).autoscale()\n",
    "\n",
    "\n",
    "# Separate Axes\n",
    "# Create a 1 row by 3 column set of axes and address a data set to each for plotting\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# new_cases_rolling_average[['China']].plot(ax=axes[0], style='-.c')\n",
    "# new_cases_rolling_average[['Italy', 'Spain']].plot(\n",
    "#     ax=axes[1], style=['-', '--'], \n",
    "#     title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n",
    "# )\n",
    "# new_cases_rolling_average[['Brazil', 'India', 'USA']]\\\n",
    "#     .plot(ax=axes[2], style=['--', ':', '-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area plot\n",
    "\n",
    "# new_cases_rolling_average.drop(columns=cols).plot(\n",
    "#     kind='area', figsize=(15, 5), \n",
    "#     title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.2 X-Y Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Y = Kind = Scatter\n",
    "# Note, simply use 'logx=True' to make x axis logerithmic\n",
    "\n",
    "# fb.assign(\n",
    "#     max_abs_change=fb.high - fb.low\n",
    "# ).plot(\n",
    "#     kind='scatter', x='volume', y='max_abs_change',\n",
    "#     title='Facebook Daily High - Low vs. log(Volume Traded)', \n",
    "#     logx=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbins\n",
    "\n",
    "# fb.assign(\n",
    "#     log_volume=np.log(fb.volume),\n",
    "#     max_abs_change=fb.high - fb.low\n",
    "# ).plot(\n",
    "#     kind='hexbin'\n",
    "#     , x='log_volume'\n",
    "#     , y='max_abs_change'\n",
    "#     , title='Facebook Daily High - Low vs. log(Volume Traded)'# Optional\n",
    "#     , colormap='gray_r' # Optional\n",
    "#     , gridsize=20       # Optional\n",
    "#     , sharex=False      # Optional: we have to pass this to see the x-axis\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.3 Histograms & KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for Discrete Data \n",
    "\n",
    "# Simple:\n",
    "# fb.volume.plot(\n",
    "#     kind='hist', \n",
    "#     title='Histogram of Daily Volume Traded in Facebook Stock'\n",
    "# )\n",
    "# plt.xlabel('Volume traded') # label the x-axis (discussed in chapter 6) \n",
    "\n",
    "# More complex\n",
    "# for magtype in quakes['magType'].unique():\n",
    "#     data = quakes.query(f'magType == \"{magtype}\"')['mag']\n",
    "#     if not data.empty:\n",
    "#         data.plot(\n",
    "#             kind='hist', ax=axes, alpha=0.4, \n",
    "#             label=magtype, legend=True,\n",
    "#             title='Comparing histograms of earthquake magnitude by magType'\n",
    "#         )\n",
    "# plt.xlabel('magnitude') # label the x-axis (discussed in chapter 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Density Estimate (KDE) like a continuous Histogram\n",
    "\n",
    "# Simple\n",
    "# fb['high'].plot(\n",
    "#     kind='kde', \n",
    "#     title='KDE of Daily High Price for Facebook Stock'\n",
    "# )\n",
    "# plt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE & Histogram\n",
    "# Use Returned Axes to Plot a second variable\n",
    "\n",
    "# ax = fb['high'].plot(\n",
    "#     kind='hist', density=True, alpha=0.5\n",
    "#     )\n",
    "# fb['high'].plot(\n",
    "#     ax=ax, kind='kde', color='blue', \n",
    "#     title='Distribution of Facebook Stock\\'s Daily High Price in 2018'\n",
    "#     )\n",
    "# plt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.4 Cumulative  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF \n",
    "\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ecdf = ECDF(quakes.query('magType == \"ml\"')['mag'])\n",
    "# plt.plot(ecdf.x, ecdf.y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.5 Box Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb.iloc[:,:4].plot(\n",
    "#       kind='box'\n",
    "#     , title='Facebook OHLC Prices Box Plot'\n",
    "#     , notch=True)                           # Optional notch at 95% confident limit\n",
    "\n",
    "# plt.ylabel('price ($)') # label the x-axis (discussed in chapter 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.6 Bar Charts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind='bar'      # For vertical bars\n",
    "kind='barh'     # For horizontal bars \n",
    "# Grouped bars with multiple columns \n",
    "# Stacked bars \n",
    "# pivot.plot.bar(\n",
    "#     stacked=True, rot=0, ylabel='earthquakes', \n",
    "#     title='Earthquakes by integer magnitude and magType'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.7 Scatter Martrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit like .corr as seems to plot all columns against each other?!?!?\n",
    "# from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Default uses histograms on diagonals (ie x vs x)\n",
    "# scatter_matrix(fb, figsize=(10, 10))\n",
    "\n",
    "# To change the diagonal from histograms to KDE:\n",
    "# scatter_matrix(fb, figsize=(10, 10), diagonal='kde')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.8 Lag Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag plots let us see how the variable correlates with past observations of itself. Random data has no pattern:\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "# Random Example - no corelation\n",
    "np.random.seed(0) # make this repeatable\n",
    "lag_plot(pd.Series(np.random.random(size=200)))\n",
    "\n",
    "# Corelate Data \n",
    "lag_plot(fb.close)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.9 Autocorrelation Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data will not have any significant autocorrelation, it stays within the bounds below\n",
    "\n",
    "# from pandas.plotting import autocorrelation_plot\n",
    "# np.random.seed(0) # make this repeatable\n",
    "\n",
    "# autocorrelation_plot(pd.Series(np.random.random(size=200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But correlated data will show a correlation \n",
    "\n",
    "# autocorrelation_plot(fb.close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3.10 Bootstrap Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot helps us understand the uncertainty in our summary statistics:\n",
    "\n",
    "# from pandas.plotting import bootstrap_plot\n",
    "# fig = bootstrap_plot(fb.volume, fig=plt.figure(figsize=(10, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.0 Set-Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.1 Strip Plot & Swarm Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripplot visualize categorical data on one axis and numerical data on the other.\n",
    "\n",
    "# sns.stripplot(\n",
    "#     x='magType',\n",
    "#     y='mag',\n",
    "#     hue='tsunami',\n",
    "#     data=quakes.query('parsed_place == \"Indonesia\"')\n",
    "# )\n",
    "\n",
    "# Bee swarm plot keeps points from overlapping. \n",
    "# sns.swarmplot(\n",
    "#     x='magType',\n",
    "#     y='mag',\n",
    "#     hue='tsunami',\n",
    "#     data=quakes.query('parsed_place == \"Indonesia\"'),\n",
    "#     size=3.5 # point size\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.2 Boxen Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxenplot creates an enhanced box plot that shows additional quantiles\n",
    "\n",
    "# sns.boxenplot(\n",
    "#     x='magType', y='mag', data=quakes[['magType', 'mag']]\n",
    "# )\n",
    "# plt.title('Comparing earthquake magnitude by magType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.3 Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots combine box plots and KDEs:\n",
    "\n",
    "# fig, axes = plt.subplots(figsize=(10, 5))\n",
    "# sns.violinplot(\n",
    "#     x='magType', y='mag', data=quakes[['magType', 'mag']],  \n",
    "#     ax=axes, scale='width' # all violins have same width\n",
    "# )\n",
    "# plt.title('Comparing earthquake magnitude by magType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.4 Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to .corr shows the corelation between \n",
    "# all the variables and each other iin a matrix\n",
    "\n",
    "# sns.heatmap(\n",
    "#     fb.sort_index().assign(\n",
    "#         log_volume=np.log(fb.volume),\n",
    "#         max_abs_change=fb.high - fb.low\n",
    "#     ).corr(),\n",
    "#     annot=True, center=0, vmin=-1, vmax=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.5 Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix between a scatter plot and the above Heatmap matrix\n",
    "# Shows a scatter plot for each relationship\n",
    "\n",
    "# Simple\n",
    "# sns.pairplot(fb)\n",
    "\n",
    "# With KDE instead of scatter for main diagonal\n",
    "# And data divided into quarters and shown in different colours\n",
    "# sns.pairplot(\n",
    "#     fb.assign(quarter=lambda x: x.index.quarter),\n",
    "#     diag_kind='kde',\n",
    "#     hue='quarter'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.6 Join Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between two variables, like a scatter plot. \n",
    "# But also visualize their distributions at the same time (as a histogram or KDE)\n",
    "# KT note: this one is really cool!\n",
    "\n",
    "# sns.jointplot(\n",
    "#     x='log_volume',\n",
    "#     y='max_abs_change',\n",
    "#     data=fb.assign(\n",
    "#         log_volume=np.log(fb.volume),\n",
    "#         max_abs_change=fb.high - fb.low\n",
    "#     )\n",
    "# )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.7 Regression Plots with reg_resid_plots from viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from viz import reg_resid_plots\n",
    "# reg_resid_plots(fb_reg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.8 Multiple Regression Plots with lmplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(\n",
    "#     x='log_volume',\n",
    "#     y='max_abs_change',\n",
    "#     data=fb.assign(\n",
    "#         log_volume=np.log(fb['volume'])\n",
    "#         , max_abs_change=fb['high'] - fb['low']\n",
    "#         , quarter=lambda x: x.index.quarter\n",
    "#         # , quarter= fb.index.quarter\n",
    "#     ),\n",
    "#     col='quarter'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.9 FacetGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(\n",
    "#     quakes.query(\n",
    "#         'parsed_place.isin([\"Indonesia\", \"Papua New Guinea\"]) '\n",
    "#         'and magType == \"mb\"'\n",
    "#     ),\n",
    "#     row='tsunami',\n",
    "#     col='parsed_place',\n",
    "#     height=4\n",
    "# )\n",
    "# g = g.map(sns.histplot, 'mag', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5 Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.X Plot Hints and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'logx=True' to make x axis logarithmic\n",
    "\n",
    "# Use 'alpha=' to make plots transparent\n",
    "\n",
    "# .plot(\n",
    "#     kind='scatter', x='volume', y='max_abs_change',\n",
    "#     title='Facebook Daily High - Low vs. log(Volume Traded)', \n",
    "#     logx=True, alpha=0.25\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other hits and tips\n",
    "\n",
    "# cols = [\n",
    "#     col for col in new_cases_rolling_average.columns \n",
    "#     if col not in ['USA', 'Brazil', 'India', 'Italy & Spain']\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Common Problems  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.1 \"SettingWithCopyWarning\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"SettingWithCopyWarning\" fixed with .copy() \n",
    "# to explicitly decare that new opbject is a copy not a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX.  To File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({      # Really nice way to gather summary stats for targeted rows into a data frame\n",
    "#     'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n",
    "#     '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n",
    "# }).T                # Note the .T to transpose the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need somthing on np.vectorize() to vectorize functions similar to how `map()` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipes = 'put result into fillowing function'\n",
    "# We can use pipes to apply any function that accepts our data as the first argument and pass in any additional arguments. This makes it easy to chain steps together regardless of whether they are methods or functions:\n",
    "\n",
    "# f(g(h(data), 20), x=True)\n",
    "# = same as =\n",
    "# data.pipe(h)\\\n",
    "#     .pipe(g, 20)\\\n",
    "#     .pipe(f, x=True)\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little function to get row counts\n",
    "# def get_row_count(*dfs):\n",
    "#     return [df.shape[0] for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify how floats are formatted for displaying. \n",
    "# Display floats with 2 digits after the decimal point:\n",
    "\n",
    "# pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivots and Cross Tabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivots and Cross Tabs\n",
    "\n",
    "# Simplest form we provide a column to place along the columns:\n",
    "# fb.pivot_table(columns='trading_volume')\n",
    "# fb.pivot_table(index='trading_volume')\n",
    "\n",
    "# weather.reset_index().pivot_table(\n",
    "#     index=['date', 'station', 'station_name'], \n",
    "#     columns='datatype', \n",
    "#     values='value',\n",
    "#     aggfunc='median'\n",
    "# ).reset_index().tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the `pd.crosstab()` function to create a frequency table.\n",
    "# pd.crosstab(\n",
    "#     index=fb.trading_volume,\n",
    "#     columns=fb.index.month,\n",
    "#     colnames=['month'],\n",
    "#     normalize='columns'           # Optional to change counts to percent\n",
    "# )\n",
    "\n",
    "\n",
    "# Or more generally than a count, e.g., a mean\\\n",
    "# pd.crosstab(\n",
    "#     index=fb.trading_volume,\n",
    "#     columns=fb.index.month,\n",
    "#     colnames=['month'],\n",
    "#     values=fb.close,\n",
    "#     aggfunc=np.mean\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints anf Tips\n",
    "\n",
    "# calculate the correlation matrix\n",
    "fb_corr = fb.assign(\n",
    "    log_volume=np.log(fb.volume),\n",
    "    max_abs_change=fb.high - fb.low\n",
    ").corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always use lambda with apply (or pass in a def function)???\n",
    "# Lambda not needed with assign???\n",
    "\n",
    "# As seen above, we can assign multiple columns in the same statement \n",
    "# and with a lambda function you can even assign new columns and reference them immediately.\n",
    "\n",
    "chicken.assign(\n",
    "    weight_kg2 = lambda row: row['weight_kg'] ** 2\n",
    "    double_height = lambda row: row['height_cm'] * 2\n",
    "    ).loc(lambda row: row['age_weeks'] <= 10)\n",
    "\n",
    "# From official doco on assign:\n",
    "# \"Alternatively, the same behavior can be achieved by directly referencing \n",
    "# an existing Series or sequence\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('conda_3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b9aa4b47fde6b9659f2be704d3beb3fd8a605c3f7f9db8b3f63f09d360b7471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
